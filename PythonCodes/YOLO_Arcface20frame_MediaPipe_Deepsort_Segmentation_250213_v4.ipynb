{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<변경점>  \n",
    "Arcface 사람별로 첫 20frame만 적용  \n",
    "segmentation으로 사람 경계선 땀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 사람 + 총 + 칼 인식    \n",
    "2. Arcface를 모든 사람에 대해 적용(Me, NotME). 첫 20프레임만  \n",
    "3. NotMe이면서 사람과 총 혹은 칼의 bounding box가 겹치는 경우에는 위험인으로 분류 -> 한 번 위험인으로 분류된 사람의 ID는 dangerous_ids로 관리됨 -> dangerous_ids 중 하나에 해당하는 사람은 위에 \"Dangerous person\"이라고 뜸\n",
    "4. pose estimation을 dangerous person에 대해 적용(왼팔들기, 오른팔들기, 양팔들기)  \n",
    "5. 출력화면 사이즈를 키움  \n",
    "6. 버그수정: 초기에 무기와 교차하여 dangerous_ids에 들어갔더라도, ArcFace로 내 얼굴임이 확인되면(=나 자신이 무기를 소지한 상황) 그 사람을 위험 인물에서 제외  \n",
    "7. Kobukki robot이 dangerous_ids 내에 있는 사람을 쫓아가도록 하면 될 듯\n",
    "8. warning 해결: pip install --upgrade albumentations\n",
    "9. 사람인 경우에 segmentation으로 윤곽선 치기(성공)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_s\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_s\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_s\\det_500m.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_s\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_s\\w600k_mbf.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "내 얼굴 평균 임베딩 생성 완료.\n"
     ]
    }
   ],
   "source": [
    "#1. 얼굴 학습시키는 부분\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "def initialize_arcface():\n",
    "    app = FaceAnalysis(name=\"buffalo_s\")  # ArcFace 모델 (buffalo_s은 기본 권장)\n",
    "    app.prepare(ctx_id=-1, det_size=(640, 640))  # GPU: ctx_id=0, CPU: -1\n",
    "    return app\n",
    "\n",
    "def get_face_embedding(app, image_bgr):\n",
    "    # ArcFace의 app.get()은 BGR 형식으로 이미지를 받기도 합니다.\n",
    "    # 만약 RGB가 필요하면 cvtColor로 변환하세요.\n",
    "    faces = app.get(image_bgr)\n",
    "    if len(faces) > 0:\n",
    "        return faces[0].embedding  # 첫 번째 얼굴의 임베딩\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_average_embedding(app, folder_path):\n",
    "    embeddings = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                print(f\"이미지 로드 실패: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            embedding = get_face_embedding(app, image)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "            else:\n",
    "                print(f\"얼굴 검출 실패: {img_path}\")\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"임베딩을 하나도 생성하지 못했습니다.\")\n",
    "    \n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    return avg_embedding\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = initialize_arcface()\n",
    "    # 내 얼굴 사진 폴더\n",
    "    my_face_folder = \"C:/Users/idea0/EE101/Jongsul/myface\"  \n",
    "    my_face_embedding = generate_average_embedding(app, my_face_folder)\n",
    "    np.save(\"my_face_embedding.npy\", my_face_embedding)  # 필요 시 저장\n",
    "    print(\"내 얼굴 평균 임베딩 생성 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_s\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_s\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_s\\det_500m.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_s\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_s\\w600k_mbf.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 105.0ms\n",
      "Speed: 4.0ms preprocess, 105.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 127.0ms\n",
      "Speed: 2.0ms preprocess, 127.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 112.0ms\n",
      "Speed: 4.0ms preprocess, 112.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 126.0ms\n",
      "Speed: 3.0ms preprocess, 126.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 107.0ms\n",
      "Speed: 3.0ms preprocess, 107.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 130.0ms\n",
      "Speed: 2.0ms preprocess, 130.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 100.0ms\n",
      "Speed: 5.0ms preprocess, 100.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.0ms\n",
      "Speed: 3.0ms preprocess, 136.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 100.0ms\n",
      "Speed: 10.0ms preprocess, 100.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 3.0ms preprocess, 135.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 90.0ms\n",
      "Speed: 23.0ms preprocess, 90.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 144.0ms\n",
      "Speed: 6.0ms preprocess, 144.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 105.0ms\n",
      "Speed: 3.0ms preprocess, 105.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 151.0ms\n",
      "Speed: 3.0ms preprocess, 151.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 183.0ms\n",
      "Speed: 32.0ms preprocess, 183.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 174.0ms\n",
      "Speed: 4.0ms preprocess, 174.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 128.0ms\n",
      "Speed: 14.0ms preprocess, 128.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 155.0ms\n",
      "Speed: 3.0ms preprocess, 155.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 114.1ms\n",
      "Speed: 6.0ms preprocess, 114.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 146.0ms\n",
      "Speed: 3.0ms preprocess, 146.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 101.0ms\n",
      "Speed: 18.0ms preprocess, 101.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 139.0ms\n",
      "Speed: 4.0ms preprocess, 139.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 91.0ms\n",
      "Speed: 11.0ms preprocess, 91.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 143.0ms\n",
      "Speed: 5.0ms preprocess, 143.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 107.0ms\n",
      "Speed: 19.0ms preprocess, 107.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 133.0ms\n",
      "Speed: 9.0ms preprocess, 133.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 114.0ms\n",
      "Speed: 8.0ms preprocess, 114.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 3.0ms preprocess, 138.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 108.0ms\n",
      "Speed: 9.0ms preprocess, 108.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 189.0ms\n",
      "Speed: 2.0ms preprocess, 189.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 102.0ms\n",
      "Speed: 3.0ms preprocess, 102.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 3.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 98.0ms\n",
      "Speed: 21.0ms preprocess, 98.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 129.0ms\n",
      "Speed: 2.0ms preprocess, 129.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 117.0ms\n",
      "Speed: 22.0ms preprocess, 117.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 132.0ms\n",
      "Speed: 2.0ms preprocess, 132.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 102.0ms\n",
      "Speed: 10.0ms preprocess, 102.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 129.0ms\n",
      "Speed: 3.0ms preprocess, 129.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 96.0ms\n",
      "Speed: 1.0ms preprocess, 96.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 131.0ms\n",
      "Speed: 6.0ms preprocess, 131.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "running Arcface...\n",
      "\n",
      "0: 480x640 1 personnnn, 110.5ms\n",
      "Speed: 22.7ms preprocess, 110.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 134.0ms\n",
      "Speed: 1.0ms preprocess, 134.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 99.0ms\n",
      "Speed: 2.0ms preprocess, 99.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 146.0ms\n",
      "Speed: 2.0ms preprocess, 146.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 104.0ms\n",
      "Speed: 5.0ms preprocess, 104.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 123.0ms\n",
      "Speed: 4.0ms preprocess, 123.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 102.0ms\n",
      "Speed: 3.0ms preprocess, 102.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 3.0ms preprocess, 142.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 95.0ms\n",
      "Speed: 2.0ms preprocess, 95.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 163.0ms\n",
      "Speed: 2.0ms preprocess, 163.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 101.1ms\n",
      "Speed: 3.0ms preprocess, 101.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 132.0ms\n",
      "Speed: 4.0ms preprocess, 132.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 101.0ms\n",
      "Speed: 2.0ms preprocess, 101.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 133.0ms\n",
      "Speed: 3.0ms preprocess, 133.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 98.0ms\n",
      "Speed: 1.0ms preprocess, 98.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 127.0ms\n",
      "Speed: 2.0ms preprocess, 127.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 119.0ms\n",
      "Speed: 3.0ms preprocess, 119.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 140.0ms\n",
      "Speed: 5.0ms preprocess, 140.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 92.0ms\n",
      "Speed: 1.0ms preprocess, 92.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 127.0ms\n",
      "Speed: 3.0ms preprocess, 127.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 99.0ms\n",
      "Speed: 1.0ms preprocess, 99.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 3.0ms preprocess, 135.0ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 95.0ms\n",
      "Speed: 4.0ms preprocess, 95.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 132.0ms\n",
      "Speed: 3.0ms preprocess, 132.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from insightface.app import FaceAnalysis\n",
    "# Deep SORT\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# 버전 업그레이드 권장 무시\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "############################\n",
    "# 1) 모델 및 함수 초기화\n",
    "############################\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# (A) YOLO **세그멘테이션** 모델 (사람=0, 총=1, 칼=2)\n",
    "#model = YOLO(\"/home/dev/runs/detect/train55/weights/epoch180.pt\").to(device)\n",
    "model = YOLO(\"C:/Users/idea0/EE101/Jongsul/Yolomodels/epoch180.pt\").to(device)\n",
    "\n",
    "model_seg = YOLO(\"yolov8n-seg.pt\").to(device)\n",
    "\n",
    "#face_model = YOLO(\"/home/dev/runs/detect/train34/weights/best.pt\").to(device)\n",
    "face_model = YOLO(\"C:/Users/idea0/EE101/Jongsul/Yolomodels/best.pt\").to(device)\n",
    "\n",
    "# ArcFace\n",
    "arc_app = FaceAnalysis(name=\"buffalo_s\")\n",
    "arc_app.prepare(ctx_id=-1, det_size=(640,640))\n",
    "my_face_embedding = np.load(\"my_face_embedding.npy\")\n",
    "\n",
    "def get_face_embedding(arc_app, face_img_bgr):\n",
    "    faces = arc_app.get(face_img_bgr)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    return faces[0].embedding\n",
    "\n",
    "def is_my_face(face_embedding, my_embedding, threshold=0.4):\n",
    "    sim = cosine_similarity([face_embedding], [my_embedding])[0][0]\n",
    "    return (sim > threshold), sim\n",
    "\n",
    "# Pose: Dangerous person만 크롭 영상에 대해 Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_danger = mp_pose.Pose(\n",
    "    static_image_mode=True,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 팔 들기 판별용(부위 인덱스)\n",
    "LEFT_SHOULDER = 11\n",
    "RIGHT_SHOULDER = 12\n",
    "LEFT_WRIST = 15\n",
    "RIGHT_WRIST = 16\n",
    "\n",
    "def is_arm_raised(shoulder_y, wrist_y, threshold=0.05):\n",
    "    return wrist_y < (shoulder_y - threshold)\n",
    "\n",
    "# 바운딩박스 overlap\n",
    "def boxes_overlap(boxA, boxB):\n",
    "    (x1A, y1A, x2A, y2A) = boxA\n",
    "    (x1B, y1B, x2B, y2B) = boxB\n",
    "    overlap_x = not (x2A < x1B or x2B < x1A)\n",
    "    overlap_y = not (y2A < y1B or y2B < y1A)\n",
    "    return overlap_x and overlap_y\n",
    "\n",
    "############################\n",
    "# 2) DeepSORT 초기화\n",
    "############################\n",
    "tracker = DeepSort(max_age=30,\n",
    "                   n_init=3,\n",
    "                   nms_max_overlap=1.0,\n",
    "                   embedder='mobilenet',\n",
    "                   half=True,\n",
    "                   embedder_gpu=True)\n",
    "\n",
    "############################\n",
    "# 3) 메인 루프\n",
    "############################\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "prev_time = time.time()\n",
    "\n",
    "# 위험 인물(무기와 교차 & 내가 아닌) track_id\n",
    "dangerous_ids = set()\n",
    "sim = 0\n",
    "\n",
    "# --- (A) ArcFace 저장 구조: 각 트랙별 ---\n",
    "track_me_status = {}       # tid -> True/False/None\n",
    "track_arcface_count = {}   # tid -> 몇 번 ArcFace 시도했나\n",
    "MAX_ARCFACE_FRAMES = 20    # 최대 ArcFace 적용 프레임 수\n",
    "\n",
    "# window resize\n",
    "window_name = \"DeepSORT + YOLO(SEG) + ArcFace + Pose(DangerOnly)\"\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name, 960, 720)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임 읽기 실패!\")\n",
    "        break\n",
    "\n",
    "    # (1) YOLO 세그멘테이션 추론\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = model(rgb_frame , conf=0.5)  # conf threshold = 0.5\n",
    "    results_seg = model_seg(rgb_frame, conf=0.5)\n",
    "\n",
    "    # YOLOv8 세그멘테이션 결과\n",
    "    det = results[0]\n",
    "    boxes2 = det.boxes\n",
    "    masks2 = results_seg[0].masks  # 세그멘테이션 마스크 정보(각 객체별)\n",
    "\n",
    "    person_detections = []\n",
    "    weapon_boxes = []\n",
    "\n",
    "    if boxes2 is not None:\n",
    "        for i, box in enumerate(boxes2):\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            class_id = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "\n",
    "            # 바운딩박스 시각화\n",
    "            cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "            label = f\"{model.names[class_id]}: {conf:.2f}\"\n",
    "            cv2.putText(frame, label, (x1,y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2)\n",
    "\n",
    "            # (A) 세그멘테이션 윤곽선 그리기 (사람=0, 총=1, 칼=2)\n",
    "            if masks2 is not None and class_id in [0,1,2]:\n",
    "                single_mask = masks2.data[i].cpu().numpy()  # shape=(H,W)\n",
    "                mask_bin = (single_mask > 0.5).astype(np.uint8)\n",
    "                contours, _ = cv2.findContours(mask_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "                # 클래스별 색상 예시\n",
    "                if class_id == 0:   # person\n",
    "                    color_seg = (0,255,255)\n",
    "                elif class_id == 1: # gun\n",
    "                    color_seg = (0,0,255)\n",
    "                elif class_id == 2: # knife\n",
    "                    color_seg = (255,0,0)\n",
    "                else:\n",
    "                    color_seg = (255,255,255)\n",
    "\n",
    "                cv2.drawContours(frame, contours, -1, color_seg, 2)\n",
    "\n",
    "            # (B) 바운딩박스 정보 → DeepSORT\n",
    "            if class_id == 0:\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "                person_detections.append(((x1,y1,w,h), conf, 0))\n",
    "            elif class_id in [1,2]:\n",
    "                weapon_boxes.append((x1,y1,x2,y2))\n",
    "\n",
    "    # (2) DeepSORT update\n",
    "    tracks = tracker.update_tracks(person_detections, frame=rgb_frame)\n",
    "    tracked_boxes = []\n",
    "    for t in tracks:\n",
    "        if not t.is_confirmed() or t.time_since_update>1:\n",
    "            continue\n",
    "        tid = t.track_id\n",
    "        l,t_,r,b_ = map(int, t.to_ltrb())\n",
    "        tracked_boxes.append((tid, l,t_,r,b_))\n",
    "        cv2.putText(frame, f\"ID:{tid}\", (l-10,t_-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255),2)\n",
    "\n",
    "    # 3) ArcFace: 각 트랙에 대해 최대 10프레임만 시도\n",
    "    for (tid, px1, py1, px2, py2) in tracked_boxes:\n",
    "        # (A) 초기화\n",
    "        if tid not in track_me_status:\n",
    "            track_me_status[tid] = False  # 아직 확정X\n",
    "        if tid not in track_arcface_count:\n",
    "            track_arcface_count[tid] = 0  # ArcFace 시도 횟수\n",
    "\n",
    "        # 현재 me_status\n",
    "        me_status = track_me_status[tid]\n",
    "\n",
    "        # 이미 me_status가 True면 더 이상 ArcFace X\n",
    "        # 아니면 ArcFace 시도 횟수 < MAX_ARCFACE_FRAMES인 경우에만 시도\n",
    "        if me_status is False and track_arcface_count[tid] < MAX_ARCFACE_FRAMES:\n",
    "            track_arcface_count[tid] += 1\n",
    "            print(\"running Arcface...\")\n",
    "\n",
    "            # Face detection\n",
    "            px1_clamped = max(0, px1)\n",
    "            face_results = face_model(frame[py1:py2, px1_clamped:px2], verbose=False)\n",
    "\n",
    "            person_crop = None\n",
    "            if len(face_results) > 0 and len(face_results[0].boxes) > 0:\n",
    "                for fbox in face_results[0].boxes:\n",
    "                    if int(fbox.cls) == 2:  # 가정: face_model에서 cls=2가 'face'\n",
    "                        fx1, fy1, fx2, fy2 = map(int, fbox.xyxy[0])\n",
    "                        person_crop = frame[py1+fy1:py1+fy2, px1_clamped+fx1:px1_clamped+fx2]\n",
    "                        break\n",
    "\n",
    "            if person_crop is not None and person_crop.size>0:\n",
    "                face_embedding = get_face_embedding(arc_app, person_crop)\n",
    "                if face_embedding is not None:\n",
    "                    same_person, sim = is_my_face(face_embedding, my_face_embedding, threshold=0.4)\n",
    "                    if same_person:\n",
    "                        track_me_status[tid] = True\n",
    "\n",
    "     \n",
    "        if me_status:\n",
    "            text_arc= f\"             Me(sim={sim:.2f})\"\n",
    "            color=(0,255,0)\n",
    "            if tid in dangerous_ids:             # 이미 me가 되었으니 dangerous_ids에서 제거\n",
    "                dangerous_ids.remove(tid)\n",
    "        else:\n",
    "            text_arc= f\"             NotMe(sim={sim:.2f})\"\n",
    "            color=(0,0,255)\n",
    "            \n",
    "        cv2.putText(frame, text_arc, (px1, py1-10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color,2)\n",
    "\n",
    "        # else: 이미 True/False이거나 시도 횟수 끝 -> 아무것도 안함\n",
    "        \n",
    "    # (4) 무기 교차 & 내가 아닌 => dangerous_ids\n",
    "    for (tid, px1, py1, px2, py2) in tracked_boxes:\n",
    "        if not track_me_status[tid]:\n",
    "            pbox = (px1, py1, px2, py2)\n",
    "            for wb in weapon_boxes:\n",
    "                if boxes_overlap(pbox, wb):\n",
    "                    dangerous_ids.add(tid)\n",
    "                    break\n",
    "\n",
    "    # (5) Dangerous => Pose\n",
    "    for (tid, px1, py1, px2, py2) in tracked_boxes:\n",
    "        if tid in dangerous_ids:\n",
    "            sub = frame[py1:py2, px1:px2]\n",
    "            if sub.size==0:\n",
    "                continue\n",
    "            c_rgb = cv2.cvtColor(sub, cv2.COLOR_BGR2RGB)\n",
    "            pose_result = pose_danger.process(c_rgb)\n",
    "            if pose_result.pose_landmarks:\n",
    "                lms = pose_result.pose_landmarks.landmark\n",
    "                sub_w = px2 - px1\n",
    "                sub_h = py2 - py1\n",
    "\n",
    "                left_shoulder_y = lms[11].y\n",
    "                right_shoulder_y = lms[12].y\n",
    "                left_wrist_y = lms[15].y\n",
    "                right_wrist_y = lms[16].y\n",
    "\n",
    "                left_arm_up = (left_wrist_y< (left_shoulder_y-0.05))\n",
    "                right_arm_up= (right_wrist_y<(right_shoulder_y-0.05))\n",
    "                action_text=\"\"\n",
    "                if left_arm_up and right_arm_up:\n",
    "                    action_text=\"both arms up\"\n",
    "                elif left_arm_up:\n",
    "                    action_text=\"left arm up\"\n",
    "                elif right_arm_up:\n",
    "                    action_text=\"right arm up\"\n",
    "                else:\n",
    "                    action_text=\"do nothing\"\n",
    "\n",
    "                for lm in lms:\n",
    "                    cx = px1+int(lm.x*sub_w)\n",
    "                    cy = py1+int(lm.y*sub_h)\n",
    "                    cv2.circle(frame,(cx,cy),3,(0,255,255),-1)\n",
    "                cv2.putText(frame, f\"Dangerous person: {action_text}\",\n",
    "                            (px1, py1+20), cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,0,255),2)\n",
    "\n",
    "    # FPS\n",
    "    now = time.time()\n",
    "    fps = 1.0/(now - prev_time)\n",
    "    prev_time=now\n",
    "    cv2.putText(frame,f\"FPS:{fps:.2f}\",(10,30),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "\n",
    "    cv2.imshow(window_name,frame)\n",
    "    if cv2.waitKey(1)&0xFF==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: None\n",
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
