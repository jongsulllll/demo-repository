{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 사람 + 총 + 칼 인식    \n",
    "2. Arcface를 모든 사람에 대해 적용(Me, NotME)  \n",
    "3. NotMe이면서 사람과 총 혹은 칼의 bounding box가 겹치는 경우에는 위험인으로 분류 -> 한 번 위험인으로 분류된 사람의 ID는 dangerous_ids로 관리됨 -> dangerous_ids 중 하나에 해당하는 사람은 위에 \"Dangerous person\"이라고 뜸\n",
    "4. pose estimation을 dangerous person에 대해 적용(왼팔들기, 오른팔들기, 양팔들기)  \n",
    "5. 출력화면 사이즈를 키움  \n",
    "6. 버그수정: 초기에 무기와 교차하여 dangerous_ids에 들어갔더라도, ArcFace로 내 얼굴임이 확인되면(=나 자신이 무기를 소지한 상황) 그 사람을 위험 인물에서 제외  \n",
    "7. Kobukki robot이 dangerous_ids 내에 있는 사람을 쫓아가도록 하면 될 듯  \n",
    "\n",
    "+ Yolo segmentation까지 추가\n",
    "+ 현재 YOLO를 사용하고 있으므로, YOLOv8-seg 모델을 사용하는 것이 가장 간단함.\n",
    "\n",
    "총이나 칼을 들고 있는 손의 정확한 위치를 파악해서 정밀하게 무기를 감지할 수 있음\n",
    "옷 색깔이나 패턴을 인식해서 사람 구별 정확도 높아짐\n",
    "제스처 구분 정확도 높아짐 \n",
    "YOLOv8-seg 모델 사용\n",
    "\n",
    "\n",
    "- v2: seg 모델 학습되어 있는 거 가져와서 테스트 (이전에 학습시켜두었던 yolo 모델은 가져오지 않음). seg + arcface\n",
    "- v3: seg + arcface + deepsort (사람)\n",
    "- v4: seg(사람) + arcface + deepsort + detection(사람,총,칼)\n",
    "detection은 이전에 학습시켰던 epoch180.pt 사용\n",
    "detection먼저 하고 사람인 경우에만 seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내 얼굴 평균 임베딩 생성 완료.\n"
     ]
    }
   ],
   "source": [
    "#1. 얼굴 학습시키는 부분\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "def initialize_arcface():\n",
    "    app = FaceAnalysis(name=\"buffalo_l\")  # ArcFace 모델 (buffalo_l은 기본 권장)\n",
    "    app.prepare(ctx_id=-1, det_size=(640, 640))  # GPU: ctx_id=0, CPU: -1\n",
    "    return app\n",
    "\n",
    "def get_face_embedding(app, image_bgr):\n",
    "    # ArcFace의 app.get()은 BGR 형식으로 이미지를 받기도 합니다.\n",
    "    # 만약 RGB가 필요하면 cvtColor로 변환하세요.\n",
    "    faces = app.get(image_bgr)\n",
    "    if len(faces) > 0:\n",
    "        return faces[0].embedding  # 첫 번째 얼굴의 임베딩\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_average_embedding(app, folder_path):\n",
    "    embeddings = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                print(f\"이미지 로드 실패: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            embedding = get_face_embedding(app, image)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "            else:\n",
    "                print(f\"얼굴 검출 실패: {img_path}\")\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"임베딩을 하나도 생성하지 못했습니다.\")\n",
    "    \n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    return avg_embedding\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = initialize_arcface()\n",
    "    # 내 얼굴 사진 폴더\n",
    "    my_face_folder = \"C:/myface\"  \n",
    "    my_face_embedding = generate_average_embedding(app, my_face_folder)\n",
    "    np.save(\"my_face_embedding.npy\", my_face_embedding)  # 필요 시 저장\n",
    "    print(\"내 얼굴 평균 임베딩 생성 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "\n",
      "0: 480x640 3 personnnns, 124.0ms\n",
      "Speed: 10.4ms preprocess, 124.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 personnnns, 79.6ms\n",
      "Speed: 3.6ms preprocess, 79.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 personnnns, 76.4ms\n",
      "Speed: 3.0ms preprocess, 76.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 2 persons, 143.0ms\n",
      "Speed: 2.8ms preprocess, 143.0ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x416 (no detections), 131.1ms\n",
      "Speed: 10.3ms preprocess, 131.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 608x640 (no detections), 159.6ms\n",
      "Speed: 21.4ms preprocess, 159.6ms inference, 8.0ms postprocess per image at shape (1, 3, 608, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 4 personnnns, 91.5ms\n",
      "Speed: 8.1ms preprocess, 91.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 2 persons, 135.0ms\n",
      "Speed: 7.4ms preprocess, 135.0ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x416 1 person, 94.3ms\n",
      "Speed: 7.7ms preprocess, 94.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 608x640 (no detections), 129.6ms\n",
      "Speed: 12.6ms preprocess, 129.6ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 480x640 2 personnnns, 82.6ms\n",
      "Speed: 20.1ms preprocess, 82.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 person, 104.0ms\n",
      "Speed: 7.6ms preprocess, 104.0ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x416 (no detections), 98.9ms\n",
      "Speed: 31.6ms preprocess, 98.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 608x640 1 cup, 158.3ms\n",
      "Speed: 24.0ms preprocess, 158.3ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 78.9ms\n",
      "Speed: 18.3ms preprocess, 78.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 person, 102.1ms\n",
      "Speed: 2.0ms preprocess, 102.1ms inference, 8.1ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (416,530) into shape (416,525)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 117\u001b[0m\n\u001b[0;32m    115\u001b[0m mask_binary \u001b[38;5;241m=\u001b[39m (mask_resized \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m    116\u001b[0m color_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(person_crop, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m--> 117\u001b[0m \u001b[43mcolor_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m mask_binary \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m  \u001b[38;5;66;03m# 초록색 채널\u001b[39;00m\n\u001b[0;32m    118\u001b[0m person_crop \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39maddWeighted(person_crop, \u001b[38;5;241m1\u001b[39m, color_mask, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    119\u001b[0m frame[y1:y2, x1:x2] \u001b[38;5;241m=\u001b[39m person_crop\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (416,530) into shape (416,525)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import mediapipe as mp\n",
    "\n",
    "############################\n",
    "# 1) 모델 및 함수 초기화\n",
    "############################\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# YOLO Detection 모델 로드\n",
    "detection_model = YOLO(\"C:/epoch180.pt\").to(device)\n",
    "\n",
    "# YOLO Segmentation 모델 로드\n",
    "segmentation_model = YOLO(\"yolov8n-seg.pt\").to(device)\n",
    "\n",
    "# DeepSORT 초기화\n",
    "tracker = DeepSort(max_age=30, n_init=3, embedder='mobilenet', half=True, embedder_gpu=True)\n",
    "\n",
    "# ArcFace 초기화\n",
    "arc_app = FaceAnalysis(name=\"buffalo_l\")\n",
    "arc_app.prepare(ctx_id=-1, det_size=(640, 640))  # ctx_id=-1: CPU, ctx_id=0: GPU\n",
    "my_face_embedding = np.load(\"my_face_embedding.npy\")  # 내 얼굴의 평균 임베딩\n",
    "\n",
    "# MediaPipe Pose Estimation 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_model = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "def get_face_embedding(face_image, arc_app):\n",
    "    faces = arc_app.get(face_image)\n",
    "    if len(faces) > 0:\n",
    "        return faces[0].embedding\n",
    "    return None\n",
    "\n",
    "def is_my_face(face_embedding, my_face_embedding, threshold=0.4):\n",
    "    if face_embedding is None:\n",
    "        return False, 0.0\n",
    "    similarity = cosine_similarity([face_embedding], [my_face_embedding])[0][0]\n",
    "    return similarity > threshold, similarity\n",
    "\n",
    "############################\n",
    "# 2) 실시간 웹캡 캡처\n",
    "############################\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"웹캠을 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "window_name = \"YOLOv8 + DeepSORT + Segmentation + ArcFace + Pose\"\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name, 960, 720)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임 읽기 실패!\")\n",
    "        break\n",
    "\n",
    "    # YOLOv8 Detection 실행\n",
    "    detection_results = detection_model(frame)\n",
    "\n",
    "    # Segmentation 마스크를 위한 빈 프레임\n",
    "    mask_overlay = np.zeros_like(frame, dtype=np.uint8)\n",
    "\n",
    "    # DeepSORT 입력을 위한 검출된 객체 리스트\n",
    "    detections = []\n",
    "\n",
    "    for box in detection_results[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # 바운딩 박스 좌표\n",
    "        class_id = int(box.cls)  # 클래스 ID\n",
    "        conf = float(box.conf)  # 신뢰도 점수\n",
    "        label = f\"{detection_model.names[class_id]}: {conf:.2f}\"\n",
    "\n",
    "        # 바운딩 박스 그리기\n",
    "        color = (0, 255, 0) if class_id == 0 else (0, 0, 255)  # 사람: 초록색, 그 외: 빨간색\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "        # DeepSORT 입력 추가 (사람만)\n",
    "        if class_id == 0:\n",
    "            detections.append(((x1, y1, x2 - x1, y2 - y1), conf, 0))  # DeepSORT에 추가\n",
    "\n",
    "    # DeepSORT 추적\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    # Segmentation, ArcFace 및 Pose Estimation 적용\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 1:\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "\n",
    "        # Tracking ID 표시\n",
    "        cv2.putText(frame, f\"ID:{track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "        # Segmentation 적용 (사람만)\n",
    "        person_crop = frame[y1:y2, x1:x2]\n",
    "        if person_crop.size == 0:\n",
    "            continue\n",
    "\n",
    "        # Segmentation 실행\n",
    "        segmentation_results = segmentation_model(person_crop)\n",
    "        if segmentation_results[0].masks is not None:\n",
    "            for mask in segmentation_results[0].masks.data:\n",
    "                mask = mask.cpu().numpy()\n",
    "                mask_resized = cv2.resize(mask, (x2 - x1, y2 - y1))\n",
    "\n",
    "                # 마스크 오버레이\n",
    "                mask_binary = (mask_resized > 0.5).astype(np.uint8)\n",
    "                color_mask = np.zeros_like(person_crop, dtype=np.uint8)\n",
    "                color_mask[:, :, 1] = mask_binary * 255  # 초록색 채널\n",
    "                person_crop = cv2.addWeighted(person_crop, 1, color_mask, 0.5, 0)\n",
    "                frame[y1:y2, x1:x2] = person_crop\n",
    "\n",
    "        # ArcFace 얼굴 인식\n",
    "        face_embedding = get_face_embedding(person_crop, arc_app)\n",
    "        is_me, similarity = is_my_face(face_embedding, my_face_embedding, threshold=0.4)\n",
    "\n",
    "        if is_me:\n",
    "            arcface_text = f\"Me (sim={similarity:.2f})\"\n",
    "            cv2.putText(frame, arcface_text, (x1, y1 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        else:\n",
    "            arcface_text = f\"Not Me (sim={similarity:.2f})\"\n",
    "            cv2.putText(frame, arcface_text, (x1, y1 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Pose Estimation (왼팔, 오른팔 감지)\n",
    "        crop_rgb = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)\n",
    "        pose_result = pose_model.process(crop_rgb)\n",
    "\n",
    "        if pose_result.pose_landmarks:\n",
    "            landmarks = pose_result.pose_landmarks.landmark\n",
    "            left_wrist_y = landmarks[15].y  # 왼손\n",
    "            right_wrist_y = landmarks[16].y  # 오른손\n",
    "            left_shoulder_y = landmarks[11].y  # 왼쪽 어깨\n",
    "            right_shoulder_y = landmarks[12].y  # 오른쪽 어깨\n",
    "\n",
    "            action_text = \"\"\n",
    "            if left_wrist_y < left_shoulder_y - 0.05:\n",
    "                action_text = \"Left arm up\"\n",
    "            if right_wrist_y < right_shoulder_y - 0.05:\n",
    "                action_text = \"Right arm up\"\n",
    "\n",
    "            if action_text:\n",
    "                cv2.putText(frame, action_text, (x1, y1 + 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # 화면에 표시\n",
    "    cv2.imshow(window_name, frame)\n",
    "\n",
    "    # ESC 또는 'q' 키로 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
