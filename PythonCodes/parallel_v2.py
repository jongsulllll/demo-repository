# parallel_v1: arcface + yolo + realsense
# 1. face ì„ë² ë”© ìƒì„± (ë©€í‹°í”„ë¡œì„¸ì‹± - ê° í”„ë¡œì„¸ìŠ¤ê°€ ê°œë³„ì ìœ¼ë¡œ ëª¨ë¸ ì´ˆê¸°í™”,ë³‘ë ¬ ì„ë² ë”© ì¶”ì¶œ)
# 2. realsense + yolo + mobilefacenet ë³‘ë ¬ ì²˜ë¦¬ 
# realsense í”„ë ˆì„ ìº¡ì²˜ - ë©€í‹°ìŠ¤ë ˆë”©
# yolo human detection - ë©€í‹°í”„ë¡œì„¸ì‹±
# ì–¼êµ´ ì„ë² ë”© ë¹„êµ (ë©€í‹°ìŠ¤ë ˆë”©)
# ì „ì²´ ì‹¤í–‰ - yoloëŠ” ë©€í‹°í”„ë¡œì„¸ì‹±, realsenseì™€ faceëŠ” ë©€í‹°ìŠ¤ë ˆë”©

# v2: mediapipe ì¶”ê°€, ë³‘ë ¬í™” o

# 1. ì‚¬ëŒ + ì´ + ì¹¼ ì¸ì‹   - yolo 
ì‚¬ëŒì¸ ê²½ìš° yolo segmentationê¹Œì§€ 
# 2. Arcfaceë¥¼ ëª¨ë“  ì‚¬ëŒì— ëŒ€í•´ ì ìš©(Me, NotME)  - yolo face model ì‚¬ìš©í•´ì„œ ì–¼êµ´ ë¶€ë¶„ì—ì„œë§Œ buffalo sc ì‚¬ìš©í•´ì„œ ì–¼êµ´ ë¶„ì„
# 3. NotMeì´ë©´ì„œ ì‚¬ëŒê³¼ ì´ í˜¹ì€ ì¹¼ì˜ bounding boxê°€ ê²¹ì¹˜ëŠ” ê²½ìš°ì—ëŠ” ìœ„í—˜ì¸ìœ¼ë¡œ ë¶„ë¥˜ -> í•œ ë²ˆ ìœ„í—˜ì¸ìœ¼ë¡œ ë¶„ë¥˜ëœ ì‚¬ëŒì˜ IDëŠ” dangerous_idsë¡œ ê´€ë¦¬ë¨ -> dangerous_ids ì¤‘ í•˜ë‚˜ì— í•´ë‹¹í•˜ëŠ” ì‚¬ëŒì€ ìœ„ì— "Dangerous person"ì´ë¼ê³  ëœ¸
# 4. pose estimationì„ dangerous personì— ëŒ€í•´ ì ìš©(ì™¼íŒ”ë“¤ê¸°, ì˜¤ë¥¸íŒ”ë“¤ê¸°, ì–‘íŒ”ë“¤ê¸°)  


import cv2
import numpy as np
import os
import insightface
from insightface.app import FaceAnalysis
from concurrent.futures import ProcessPoolExecutor

def init_model():
    """ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ê°œë³„ì ìœ¼ë¡œ FaceAnalysis ëª¨ë¸ì„ ì´ˆê¸°í™”"""
    global app
    app = FaceAnalysis(name="buffalo_sc")  # MobileFaceNet ê³„ì—´
    app.prepare(ctx_id=0, det_size=(640, 640))  # ğŸ”¹ GPU ëª¨ë“œ ì‚¬ìš©

def get_mobileface_embedding(image_path):
    """ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ì—¬ MobileFaceNet ì„ë² ë”©ì„ ì¶”ì¶œ"""
    img_bgr = cv2.imread(image_path)
    if img_bgr is None:
        return None
    faces = app.get(img_bgr)
    if len(faces) == 0:
        return None
    return faces[0].embedding

def process_image(file, folder):
    """ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë  í•¨ìˆ˜ (my_face_folderë¥¼ ì¸ìë¡œ ë°›ìŒ)"""
    init_model()  # ğŸ”¹ ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ MobileFaceNet ëª¨ë¸ì„ ê°œë³„ì ìœ¼ë¡œ ì´ˆê¸°í™”
    path = os.path.join(folder, file)
    if file.lower().endswith(('.jpg', '.png', '.jpeg')):
        return get_mobileface_embedding(path)
    return None

if __name__ == "__main__":
    my_face_folder = "C:/myface"
    image_files = [file for file in os.listdir(my_face_folder) if file.lower().endswith(('.jpg', '.png', '.jpeg'))]
    
    embeddings = []
    with ProcessPoolExecutor() as executor:
        results = executor.map(process_image, image_files, [my_face_folder] * len(image_files))  # ğŸ”¹ ì¸ìë¡œ ì „ë‹¬

        for emb in results:
            if emb is not None:
                embeddings.append(emb)

    if len(embeddings) == 0:
        raise ValueError("No embeddings generated from MobileFaceNet.")

    avg_embedding = np.mean(embeddings, axis=0)
    print("MobileFaceNet embedding shape:", avg_embedding.shape)
    
    np.save("my_mobileface_embedding.npy", avg_embedding)
    print("Saved my_mobileface_embedding.npy")

#####################ì—¬ê¸°ë¶€í„° ìˆ˜ì •##############################
import cv2
import numpy as np
import os
import insightface
from insightface.app import FaceAnalysis
from concurrent.futures import ProcessPoolExecutor
import mediapipe as mp
import time
import torch
import warnings
import threading
import multiprocessing as mp_process
from queue import Queue
from ultralytics import YOLO
from sklearn.metrics.pairwise import cosine_similarity
import pyrealsense2 as rs

############################
# (A) Mediapipe Pose ì²˜ë¦¬ (ë©€í‹°ìŠ¤ë ˆë”©)
############################
class MediapipePoseProcessor(threading.Thread):
    def __init__(self, input_queue, output_queue):
        super().__init__()
        self.input_queue = input_queue
        self.output_queue = output_queue
        self.mp_pose = mp.solutions.pose.Pose(static_image_mode=False,
                                               model_complexity=1,
                                               enable_segmentation=False,
                                               min_detection_confidence=0.5,
                                               min_tracking_confidence=0.5)
        self.running = True
    
    def run(self):
        while self.running:
            data = self.input_queue.get()
            if data is None:
                break
            tid, person_crop = data
            rgb_frame = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)
            result = self.mp_pose.process(rgb_frame)
            self.output_queue.put((tid, result))
    
    def stop(self):
        self.running = False

############################
# (B) RealSense í”„ë ˆì„ ìº¡ì²˜ (ë©€í‹°ìŠ¤ë ˆë”©)
############################
class RealSenseCapture(threading.Thread):
    def __init__(self, queue):
        super().__init__()
        self.queue = queue
        self.pipeline = rs.pipeline()
        config = rs.config()
        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
        self.pipeline.start(config)
        self.running = True

    def run(self):
        while self.running:
            frames = self.pipeline.wait_for_frames()
            color_frame = frames.get_color_frame()
            depth_frame = frames.get_depth_frame()
            if color_frame and depth_frame:
                self.queue.put((color_frame, depth_frame))

    def stop(self):
        self.running = False
        self.pipeline.stop()

############################
# (C) YOLOë¥¼ ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ì‹¤í–‰ (ì‚¬ëŒ + ì´ + ì¹¼ íƒì§€)
############################
def process_detection(input_queue, output_queue, model_path):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = YOLO(model_path).to(device)
    
    while True:
        frame = input_queue.get()
        if frame is None:
            break
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = model(rgb_frame, conf=0.5)
        
        det = results[0]
        boxes = det.boxes
        
        person_detections = []
        weapon_boxes = []
        for box in boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            class_id = int(box.cls[0])
            conf = float(box.conf[0])
            if class_id == 0:  # ì‚¬ëŒ íƒì§€
                w, h = x2 - x1, y2 - y1
                person_detections.append(((x1, y1, w, h), conf, 0))
            elif class_id in [1, 2]:  # ì´ ë˜ëŠ” ì¹¼ íƒì§€
                weapon_boxes.append((x1, y1, x2, y2))
        
        output_queue.put((person_detections, weapon_boxes))

############################
# (D) ìœ„í—˜ì¸ ë¶„ë¥˜ ë° ArcFace ì²˜ë¦¬
############################
dangerous_ids = set()

############################
# (E) ë©”ì¸ ì‹¤í–‰
############################
if __name__ == "__main__":
    frame_queue = Queue()
    detection_input_queue = mp_process.Queue()
    detection_output_queue = mp_process.Queue()
    face_input_queue = Queue()
    face_output_queue = Queue()
    pose_input_queue = Queue()
    pose_output_queue = Queue()
    
    # RealSense í”„ë ˆì„ ìº¡ì²˜ ì‹œì‘
    realsense_capture = RealSenseCapture(frame_queue)
    realsense_capture.start()
    
    # YOLO í”„ë¡œì„¸ìŠ¤ ì‹œì‘ (ì‚¬ëŒ + ì´ + ì¹¼ íƒì§€)
    detection_process = mp_process.Process(target=process_detection, args=(detection_input_queue, detection_output_queue, "C:/epoch180.pt"))
    detection_process.start()
    
    # Mediapipe Pose ìŠ¤ë ˆë“œ ì‹œì‘
    pose_processor = MediapipePoseProcessor(pose_input_queue, pose_output_queue)
    pose_processor.start()
    
    # Dangerous Person ë¶„ë¥˜ ë° FPS ì¶œë ¥ ë“± ê´€ë¦¬
    cv2.destroyAllWindows()
