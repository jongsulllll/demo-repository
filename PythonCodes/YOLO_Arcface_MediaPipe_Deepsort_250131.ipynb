{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사람 + 총 + 칼 인식 //\n",
    "pose estimation을 모든 사람에 대해 적용(왼팔들기, 오른팔들기, 양팔들기) //\n",
    "사람과 총 혹은 칼의 bounding box가 겹치는 경우에는 위험인으로 분류 -> 한 번 위험인으로 분류된 사람의 ID는 dangerous_ids로 관리됨 -> dangerous_ids 중 하나에 해당하는 사람은 Deepsort로 추적하여 Arcface를 적용(인증된 얼굴인지 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "내 얼굴 평균 임베딩 생성 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idea0\\AppData\\Roaming\\Python\\Python38\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    }
   ],
   "source": [
    "#1. 얼굴 학습시키는 부분\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "def initialize_arcface():\n",
    "    app = FaceAnalysis(name=\"buffalo_l\")  # ArcFace 모델 (buffalo_l은 기본 권장)\n",
    "    app.prepare(ctx_id=-1, det_size=(640, 640))  # GPU: ctx_id=0, CPU: -1\n",
    "    return app\n",
    "\n",
    "def get_face_embedding(app, image_bgr):\n",
    "    # ArcFace의 app.get()은 BGR 형식으로 이미지를 받기도 합니다.\n",
    "    # 만약 RGB가 필요하면 cvtColor로 변환하세요.\n",
    "    faces = app.get(image_bgr)\n",
    "    if len(faces) > 0:\n",
    "        return faces[0].embedding  # 첫 번째 얼굴의 임베딩\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_average_embedding(app, folder_path):\n",
    "    embeddings = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                print(f\"이미지 로드 실패: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            embedding = get_face_embedding(app, image)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "            else:\n",
    "                print(f\"얼굴 검출 실패: {img_path}\")\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"임베딩을 하나도 생성하지 못했습니다.\")\n",
    "    \n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    return avg_embedding\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = initialize_arcface()\n",
    "    # 내 얼굴 사진 폴더\n",
    "    my_face_folder = \"C:/Users/idea0/EE101/Jongsul/myface\"  \n",
    "    my_face_embedding = generate_average_embedding(app, my_face_folder)\n",
    "    np.save(\"my_face_embedding.npy\", my_face_embedding)  # 필요 시 저장\n",
    "    print(\"내 얼굴 평균 임베딩 생성 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\idea0/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 98.0ms\n",
      "Speed: 2.0ms preprocess, 98.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 108.0ms\n",
      "Speed: 3.0ms preprocess, 108.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 1 knifeeeee, 77.0ms\n",
      "Speed: 2.0ms preprocess, 77.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 1 knifeeeee, 80.0ms\n",
      "Speed: 1.0ms preprocess, 80.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 1 knifeeeee, 85.0ms\n",
      "Speed: 1.0ms preprocess, 85.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 80.0ms\n",
      "Speed: 1.0ms preprocess, 80.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 personnnns, 1 knifeeeee, 74.9ms\n",
      "Speed: 1.0ms preprocess, 74.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 personnnns, 1 knifeeeee, 90.0ms\n",
      "Speed: 2.0ms preprocess, 90.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 personnnns, 78.0ms\n",
      "Speed: 1.0ms preprocess, 78.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 75.0ms\n",
      "Speed: 2.0ms preprocess, 75.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 74.0ms\n",
      "Speed: 2.0ms preprocess, 74.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 75.0ms\n",
      "Speed: 2.0ms preprocess, 75.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 76.0ms\n",
      "Speed: 2.0ms preprocess, 76.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 74.0ms\n",
      "Speed: 2.0ms preprocess, 74.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 91.0ms\n",
      "Speed: 2.0ms preprocess, 91.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 75.0ms\n",
      "Speed: 2.0ms preprocess, 75.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 73.0ms\n",
      "Speed: 2.0ms preprocess, 73.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 74.0ms\n",
      "Speed: 1.0ms preprocess, 74.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 84.0ms\n",
      "Speed: 1.0ms preprocess, 84.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 75.0ms\n",
      "Speed: 1.0ms preprocess, 75.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 88.6ms\n",
      "Speed: 1.0ms preprocess, 88.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 personnnns, 85.0ms\n",
      "Speed: 2.0ms preprocess, 85.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 76.8ms\n",
      "Speed: 2.0ms preprocess, 76.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 75.7ms\n",
      "Speed: 2.0ms preprocess, 75.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 74.0ms\n",
      "Speed: 1.0ms preprocess, 74.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 77.7ms\n",
      "Speed: 2.0ms preprocess, 77.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 74.1ms\n",
      "Speed: 2.0ms preprocess, 74.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 75.0ms\n",
      "Speed: 2.0ms preprocess, 75.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 75.0ms\n",
      "Speed: 2.0ms preprocess, 75.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 95.8ms\n",
      "Speed: 1.0ms preprocess, 95.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 74.0ms\n",
      "Speed: 2.0ms preprocess, 74.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 79.0ms\n",
      "Speed: 1.0ms preprocess, 79.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 1 knifeeeee, 75.0ms\n",
      "Speed: 2.0ms preprocess, 75.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 71.7ms\n",
      "Speed: 2.0ms preprocess, 71.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 73.0ms\n",
      "Speed: 1.0ms preprocess, 73.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 76.0ms\n",
      "Speed: 2.0ms preprocess, 76.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 1 gunnnnn, 73.0ms\n",
      "Speed: 2.0ms preprocess, 73.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idea0\\AppData\\Roaming\\Python\\Python38\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 personnnns, 79.0ms\n",
      "Speed: 2.0ms preprocess, 79.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idea0\\AppData\\Roaming\\Python\\Python38\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 personnnns, 76.0ms\n",
      "Speed: 2.0ms preprocess, 76.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idea0\\AppData\\Roaming\\Python\\Python38\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 personnnns, 74.0ms\n",
      "Speed: 2.0ms preprocess, 74.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idea0\\AppData\\Roaming\\Python\\Python38\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 2 personnnns, 77.0ms\n",
      "Speed: 2.0ms preprocess, 77.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idea0\\AppData\\Roaming\\Python\\Python38\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 personnnn, 72.0ms\n",
      "Speed: 1.0ms preprocess, 72.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idea0\\AppData\\Roaming\\Python\\Python38\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 personnnn, 79.0ms\n",
      "Speed: 2.0ms preprocess, 79.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 78.5ms\n",
      "Speed: 2.0ms preprocess, 78.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 personnnns, 77.0ms\n",
      "Speed: 1.0ms preprocess, 77.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 72.0ms\n",
      "Speed: 1.0ms preprocess, 72.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# Deep SORT (예: deep_sort_realtime)\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "############################\n",
    "# 1) 모델 및 함수 초기화\n",
    "############################\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# (A) YOLO 모델 (사람=0, 총=1, 칼=2)\n",
    "model = YOLO(\"C:/Users/idea0/EE101/Jongsul/Yolomodels/epoch180.pt\").to(device)\n",
    "\n",
    "# (B) ArcFace\n",
    "arc_app = FaceAnalysis(name=\"buffalo_l\")\n",
    "arc_app.prepare(ctx_id=-1, det_size=(640,640))\n",
    "\n",
    "my_face_embedding = np.load(\"my_face_embedding.npy\")\n",
    "\n",
    "def get_face_embedding(arc_app, face_img_bgr):\n",
    "    faces = arc_app.get(face_img_bgr)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    return faces[0].embedding\n",
    "\n",
    "def is_my_face(face_embedding, my_embedding, threshold=0.4):\n",
    "    sim = cosine_similarity([face_embedding], [my_embedding])[0][0]\n",
    "    return (sim > threshold), sim\n",
    "\n",
    "# (C) Mediapipe Pose (전신) - 모든 사람에게 적용할 수도, 일부에 적용할 수도 있음\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Pose에서 사용할 간단한 팔 들기 판별\n",
    "LEFT_SHOULDER = 11\n",
    "RIGHT_SHOULDER = 12\n",
    "LEFT_WRIST = 15\n",
    "RIGHT_WRIST = 16\n",
    "\n",
    "def is_arm_raised(shoulder_y, wrist_y, threshold=0.05):\n",
    "    # y=0 상단, y=1 하단 (작을수록 위)\n",
    "    return wrist_y < (shoulder_y - threshold)\n",
    "\n",
    "# (D) 박스 overlap\n",
    "def boxes_overlap(boxA, boxB):\n",
    "    (x1A, y1A, x2A, y2A) = boxA\n",
    "    (x1B, y1B, x2B, y2B) = boxB\n",
    "    overlap_x = not (x2A < x1B or x2B < x1A)\n",
    "    overlap_y = not (y2A < y1B or y2B < y1A)\n",
    "    return overlap_x and overlap_y\n",
    "\n",
    "############################\n",
    "# 2) DeepSORT 초기화\n",
    "############################\n",
    "\n",
    "# deep_sort_realtime 버전에 따라 파라미터 이름이 다를 수 있음\n",
    "#tracker = DeepSort(\n",
    "#    max_age=30,\n",
    "#    n_init=2,\n",
    "#    max_iou_distance=0.95,       #default 0.7\n",
    "#    max_cosine_distance=0.95,   # <-- max_dist 대신 여기를 사용 default 0.5\n",
    "#    nn_budget=100,\n",
    "#    override_track_class=None,\n",
    "#    embedder=\"mobilenet\",\n",
    "#    half=True\n",
    "#)\n",
    "\n",
    "# DeepSort 초기화\n",
    "tracker = DeepSort(max_age=30,\n",
    "                   n_init=3,\n",
    "                   nms_max_overlap=1.0,\n",
    "                   embedder='mobilenet',\n",
    "                   half=True,\n",
    "                   embedder_gpu=True)  # GPU 사용 가능 시 True\n",
    "\n",
    "\n",
    "############################\n",
    "# 3) 메인 루프\n",
    "############################\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"카메라를 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "prev_time = time.time()\n",
    "\n",
    "# 위험 인물(무기와 교차)로 판별된 track_id 저장\n",
    "dangerous_ids = set()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임 읽기 실패!\")\n",
    "        break\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # (1) Mediapipe Pose: 전체 프레임에 대한 분석\n",
    "    # -------------------------------------------\n",
    "    rgb_pose = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pose_results = pose.process(rgb_pose)\n",
    "\n",
    "    action_text = \"\"\n",
    "    if pose_results.pose_landmarks:\n",
    "        landmarks = pose_results.pose_landmarks.landmark\n",
    "        left_shoulder_y = landmarks[LEFT_SHOULDER].y\n",
    "        right_shoulder_y = landmarks[RIGHT_SHOULDER].y\n",
    "        left_wrist_y = landmarks[LEFT_WRIST].y\n",
    "        right_wrist_y = landmarks[RIGHT_WRIST].y\n",
    "\n",
    "        left_arm_up = is_arm_raised(left_shoulder_y, left_wrist_y)\n",
    "        right_arm_up = is_arm_raised(right_shoulder_y, right_wrist_y)\n",
    "\n",
    "        if left_arm_up and right_arm_up:\n",
    "            action_text = \"both arms up\"\n",
    "        elif left_arm_up:\n",
    "            action_text = \"left arm up\"\n",
    "        elif right_arm_up:\n",
    "            action_text = \"right arm up\"\n",
    "        else:\n",
    "            action_text = \"do nothing\"\n",
    "\n",
    "        mp_drawing.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    cv2.putText(frame, action_text, (30, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # (2) YOLO 추론\n",
    "    # -------------------------------------------\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = model(rgb_frame)\n",
    "\n",
    "    person_detections = []  # -> DeepSORT에 보낼 사람 감지\n",
    "    weapon_boxes = []       # 총(1) 또는 칼(2)\n",
    "\n",
    "    for box in results[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        class_id = int(box.cls)\n",
    "        conf = float(box.conf)\n",
    "        class_name = model.names[class_id]\n",
    "\n",
    "        # 바운딩박스 시각화\n",
    "        cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "        label = f\"{class_name}: {conf:.2f}\"\n",
    "        cv2.putText(frame, label, (x1,y1-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2)\n",
    "\n",
    "        if class_id == 0:  # person\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            person_detections.append(((x1, y1, w, h), conf, 0))\n",
    "        elif class_id in [1,2]:  # gun/knife\n",
    "            weapon_boxes.append((x1,y1,x2,y2))\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # (3) DeepSORT로 사람 추적\n",
    "    # -------------------------------------------\n",
    "    tracks = tracker.update_tracks(person_detections, frame=rgb_frame)\n",
    "\n",
    "    # 임시로 저장\n",
    "    tracked_boxes = []  # (track_id, x1,y1,x2,y2)\n",
    "\n",
    "    for t in tracks:\n",
    "        if not t.is_confirmed() or t.time_since_update > 1:\n",
    "            continue\n",
    "        track_id = t.track_id\n",
    "        ltrb = t.to_ltrb()  # left, top, right, bottom\n",
    "        x1t, y1t, x2t, y2t = map(int, ltrb)\n",
    "        tracked_boxes.append((track_id, x1t, y1t, x2t, y2t))\n",
    "\n",
    "        # 시각화\n",
    "        cv2.rectangle(frame, (x1t,y1t), (x2t,y2t), (255,255,0), 2)\n",
    "        cv2.putText(frame, f\"ID:{track_id}\", (x1t,y1t-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # (4) 무기와 교차 => dangerous_ids에 추가\n",
    "    # -------------------------------------------\n",
    "    for (tid, px1, py1, px2, py2) in tracked_boxes:\n",
    "        person_box = (px1, py1, px2, py2)\n",
    "        for wb in weapon_boxes:\n",
    "            if boxes_overlap(person_box, wb):\n",
    "                dangerous_ids.add(tid)\n",
    "                break\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # (5) ArcFace: dangerous_ids에 속하는 트랙만\n",
    "    # -------------------------------------------\n",
    "    for (tid, px1, py1, px2, py2) in tracked_boxes:\n",
    "        if tid in dangerous_ids:\n",
    "            # ArcFace\n",
    "            person_crop = frame[py1:py2, px1:px2]\n",
    "            if person_crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            face_embedding = get_face_embedding(arc_app, person_crop)\n",
    "            if face_embedding is not None:\n",
    "                same_person, sim = is_my_face(face_embedding, my_face_embedding, threshold=0.4)\n",
    "                if same_person:\n",
    "                    color = (0,255,0)\n",
    "                    text_arc = f\"             Me(sim={sim:.2f})\"\n",
    "                else:\n",
    "                    color = (0,0,255)\n",
    "                    text_arc = f\"             NotMe(sim={sim:.2f})\"\n",
    "\n",
    "                cv2.rectangle(frame, (px1,py1), (px2,py2), color, 2)\n",
    "                cv2.putText(frame, text_arc, (px1, py1-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    # FPS\n",
    "    current_time = time.time()\n",
    "    fps = 1.0 / (current_time - prev_time)\n",
    "    prev_time = current_time\n",
    "    cv2.putText(frame, f\"FPS: {fps:.2f}\", (10,30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"DeepSORT + YOLO + ArcFace + Pose + Weapons\", frame)\n",
    "#    time.sleep(0.1)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
