{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 사람 + 총 + 칼 인식    \n",
    "2. Arcface를 모든 사람에 대해 적용(Me, NotME)  \n",
    "3. NotMe이면서 사람과 총 혹은 칼의 bounding box가 겹치는 경우에는 위험인으로 분류 -> 한 번 위험인으로 분류된 사람의 ID는 dangerous_ids로 관리됨 -> dangerous_ids 중 하나에 해당하는 사람은 위에 \"Dangerous person\"이라고 뜸\n",
    "4. pose estimation을 dangerous person에 대해 적용(왼팔들기, 오른팔들기, 양팔들기)  \n",
    "5. 출력화면 사이즈를 키움  \n",
    "6. 버그수정: 초기에 무기와 교차하여 dangerous_ids에 들어갔더라도, ArcFace로 내 얼굴임이 확인되면(=나 자신이 무기를 소지한 상황) 그 사람을 위험 인물에서 제외  \n",
    "7. Kobukki robot이 dangerous_ids 내에 있는 사람을 쫓아가도록 하면 될 듯  \n",
    "\n",
    "+ Yolo segmentation까지 추가\n",
    "+ 현재 YOLO를 사용하고 있으므로, YOLOv8-seg 모델을 사용하는 것이 가장 간단함.\n",
    "\n",
    "총이나 칼을 들고 있는 손의 정확한 위치를 파악해서 정밀하게 무기를 감지할 수 있음\n",
    "옷 색깔이나 패턴을 인식해서 사람 구별 정확도 높아짐\n",
    "제스처 구분 정확도 높아짐 \n",
    "YOLOv8-seg 모델 사용\n",
    "\n",
    "\n",
    "- v2: seg 모델 학습되어 있는 거 가져와서 테스트 (이전에 학습시켜두었던 yolo 모델은 가져오지 않음). seg + arcface\n",
    "- v3: seg + arcface + deepsort (사람)\n",
    "  \n",
    "- v4:\n",
    "Segmentation(사람에 한정) + Detection(사람, 총, 칼)   + arcface + deepsort\n",
    "detection은 이전에 학습시켰던 epoch180.pt 사용\n",
    "detection먼저 하고 사람인 경우에만 seg\n",
    "(사람 bounding box 안에서 segmentation 되는듯)\n",
    "\n",
    "개선할 점: 얼굴인식은 한 번만 (어차피 카메라가 목표대상을 계속 따라가기 때문에)\n",
    "- tracking id 당 한 번씩만\n",
    "\n",
    "- v5: tracking id 당 한 번씩만 얼굴 인식\n",
    "\n",
    "v4까지는 느려도 잘 되는데 v5부터는 처음부터 끊김\n",
    "\n",
    "0213v4: 0205v5에서 arcface 개선\n",
    "얼굴 인식 잘 안됨\n",
    "v5: 얼굴인식 잘 됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.4' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내 얼굴 평균 임베딩 생성 완료.\n"
     ]
    }
   ],
   "source": [
    "#1. 얼굴 학습시키는 부분\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "def initialize_arcface():\n",
    "    app = FaceAnalysis(name=\"buffalo_l\")  # ArcFace 모델 (buffalo_l은 기본 권장)\n",
    "    app.prepare(ctx_id=-1, det_size=(640, 640))  # GPU: ctx_id=0, CPU: -1\n",
    "    return app\n",
    "\n",
    "def get_face_embedding(app, image_bgr):\n",
    "    # ArcFace의 app.get()은 BGR 형식으로 이미지를 받기도 합니다.\n",
    "    # 만약 RGB가 필요하면 cvtColor로 변환하세요.\n",
    "    faces = app.get(image_bgr)\n",
    "    if len(faces) > 0:\n",
    "        return faces[0].embedding  # 첫 번째 얼굴의 임베딩\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_average_embedding(app, folder_path):\n",
    "    embeddings = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                print(f\"이미지 로드 실패: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            embedding = get_face_embedding(app, image)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "            else:\n",
    "                print(f\"얼굴 검출 실패: {img_path}\")\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"임베딩을 하나도 생성하지 못했습니다.\")\n",
    "    \n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    return avg_embedding\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = initialize_arcface()\n",
    "    # 내 얼굴 사진 폴더\n",
    "    my_face_folder = \"C:/myface\"  \n",
    "    my_face_embedding = generate_average_embedding(app, my_face_folder)\n",
    "    np.save(\"my_face_embedding.npy\", my_face_embedding)  # 필요 시 저장\n",
    "    print(\"내 얼굴 평균 임베딩 생성 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 88.9ms\n",
      "Speed: 15.7ms preprocess, 88.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 129.8ms\n",
      "Speed: 3.2ms preprocess, 129.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 86.1ms\n",
      "Speed: 2.9ms preprocess, 86.1ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 95.6ms\n",
      "Speed: 2.5ms preprocess, 95.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 1 gunnnnn, 86.6ms\n",
      "Speed: 0.0ms preprocess, 86.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 133.8ms\n",
      "Speed: 2.6ms preprocess, 133.8ms inference, 0.0ms postprocess per image at shape (1, 3, 416, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 personnnn, 1 gunnnnn, 100.8ms\n",
      "Speed: 25.3ms preprocess, 100.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 92.9ms\n",
      "Speed: 2.8ms preprocess, 92.9ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 personnnn, 1 gunnnnn, 107.4ms\n",
      "Speed: 8.5ms preprocess, 107.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 352x640 1 person, 1 tv, 117.5ms\n",
      "Speed: 2.7ms preprocess, 117.5ms inference, 0.0ms postprocess per image at shape (1, 3, 352, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 personnnn, 1 gunnnnn, 108.8ms\n",
      "Speed: 3.0ms preprocess, 108.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 92.7ms\n",
      "Speed: 2.3ms preprocess, 92.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 personnnn, 133.5ms\n",
      "Speed: 17.4ms preprocess, 133.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 146.9ms\n",
      "Speed: 3.4ms preprocess, 146.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 personnnn, 1 gunnnnn, 218.3ms\n",
      "Speed: 9.5ms preprocess, 218.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 cell phone, 227.2ms\n",
      "Speed: 6.0ms preprocess, 227.2ms inference, 10.2ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 personnnn, 198.2ms\n",
      "Speed: 19.6ms preprocess, 198.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 167.4ms\n",
      "Speed: 5.1ms preprocess, 167.4ms inference, 0.0ms postprocess per image at shape (1, 3, 416, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 personnnn, 182.4ms\n",
      "Speed: 19.3ms preprocess, 182.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 169.2ms\n",
      "Speed: 9.3ms preprocess, 169.2ms inference, 5.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 232.8ms\n",
      "Speed: 16.7ms preprocess, 232.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 1 tv, 176.3ms\n",
      "Speed: 5.4ms preprocess, 176.3ms inference, 10.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 149.5ms\n",
      "Speed: 4.0ms preprocess, 149.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 1 tv, 205.8ms\n",
      "Speed: 5.4ms preprocess, 205.8ms inference, 10.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 149.2ms\n",
      "Speed: 4.6ms preprocess, 149.2ms inference, 8.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 1 tv, 173.9ms\n",
      "Speed: 3.7ms preprocess, 173.9ms inference, 10.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 158.2ms\n",
      "Speed: 3.0ms preprocess, 158.2ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 1 tv, 198.8ms\n",
      "Speed: 5.4ms preprocess, 198.8ms inference, 0.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 148.3ms\n",
      "Speed: 5.5ms preprocess, 148.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 416x640 1 person, 1 tv, 176.9ms\n",
      "Speed: 4.6ms preprocess, 176.9ms inference, 8.0ms postprocess per image at shape (1, 3, 416, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import mediapipe as mp\n",
    "\n",
    "############################\n",
    "# 1) 모델 및 함수 초기화\n",
    "############################\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# YOLO Detection 모델 로드\n",
    "detection_model = YOLO(\"C:/epoch180.pt\").to(device)\n",
    "\n",
    "# YOLO Segmentation 모델 로드\n",
    "segmentation_model = YOLO(\"yolov8n-seg.pt\").to(device)\n",
    "\n",
    "# YOLO Face Detection 모델 로드\n",
    "face_model = YOLO(\"C:/best.pt\").to(device)\n",
    "\n",
    "# DeepSORT 초기화\n",
    "tracker = DeepSort(max_age=30, n_init=3, embedder='mobilenet', half=True, embedder_gpu=True)\n",
    "\n",
    "# ArcFace 초기화\n",
    "arc_app = FaceAnalysis(name=\"buffalo_l\")\n",
    "arc_app.prepare(ctx_id=-1, det_size=(640, 640))  # ctx_id=-1: CPU, ctx_id=0: GPU\n",
    "my_face_embedding = np.load(\"my_face_embedding.npy\")  # 내 얼굴의 평균 임베딩\n",
    "\n",
    "# MediaPipe Pose Estimation 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_model = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# ArcFace 속도 개선: 초기 10 프레임 동안 연속 실행\n",
    "INITIAL_FRAME_LIMIT = 10\n",
    "frame_count = 0\n",
    "arcface_enabled = True\n",
    "arcface_result = None  # ArcFace 결과를 저장하여 연속적으로 유지\n",
    "\n",
    "\n",
    "def get_face_embedding(face_image, arc_app):\n",
    "    faces = arc_app.get(face_image)\n",
    "    if len(faces) > 0:\n",
    "        return faces[0].embedding\n",
    "    return None\n",
    "\n",
    "def is_my_face(face_embedding, my_face_embedding, threshold=0.4):\n",
    "    if face_embedding is None:\n",
    "        return False, 0.0\n",
    "    similarity = cosine_similarity([face_embedding], [my_face_embedding])[0][0]\n",
    "    return similarity > threshold, similarity\n",
    "\n",
    "def segment_within_bboxes(image, bboxes):\n",
    "    for (x1, y1, x2, y2) in bboxes:\n",
    "        person_crop = image[y1:y2, x1:x2]\n",
    "        if person_crop.size == 0:\n",
    "            continue\n",
    "        segmentation_results = segmentation_model(person_crop)\n",
    "        if segmentation_results[0].masks is not None:\n",
    "            for mask in segmentation_results[0].masks.data:\n",
    "                mask = mask.cpu().numpy()\n",
    "                mask_resized = cv2.resize(mask, (x2 - x1, y2 - y1))\n",
    "                mask_binary = (mask_resized > 0.5).astype(np.uint8)\n",
    "                color_mask = np.zeros_like(person_crop, dtype=np.uint8)\n",
    "                color_mask[:, :, 1] = mask_binary * 255\n",
    "                person_crop = cv2.addWeighted(person_crop, 1, color_mask, 0.5, 0)\n",
    "                image[y1:y2, x1:x2] = person_crop\n",
    "    return image\n",
    "\n",
    "############################\n",
    "# 2) 실시간 웹캡 캡처\n",
    "############################\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"웹캠을 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "window_name = \"YOLOv8 + DeepSORT + Segmentation + ArcFace + Pose\"\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name, 960, 720)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임 읽기 실패!\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count > INITIAL_FRAME_LIMIT:\n",
    "        arcface_enabled = False  # 초기 10프레임 이후 ArcFace 비활성화\n",
    "    \n",
    "    detection_results = detection_model(frame)\n",
    "    bboxes = []\n",
    "    detections = []\n",
    "\n",
    "    for box in detection_results[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        class_id = int(box.cls)\n",
    "        conf = float(box.conf)\n",
    "        if class_id == 0:\n",
    "            bboxes.append((x1, y1, x2, y2))\n",
    "            detections.append(((x1, y1, x2 - x1, y2 - y1), conf, 0))\n",
    "\n",
    "    frame = segment_within_bboxes(frame, bboxes)\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 1:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "        track_id = track.track_id\n",
    "        cv2.putText(frame, f\"ID:{track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "        \n",
    "        if arcface_enabled:\n",
    "            face_embedding = get_face_embedding(frame[y1:y2, x1:x2], arc_app)\n",
    "            if face_embedding is not None:\n",
    "                arcface_result = is_my_face(face_embedding, my_face_embedding, threshold=0.35)\n",
    "        \n",
    "        is_me, similarity = arcface_result if arcface_result is not None else (False, 0.0)\n",
    "        text_color = (0, 255, 0) if is_me else (0, 0, 255)\n",
    "        arcface_text = f\"Me (sim={similarity:.2f})\" if is_me else f\"Not Me (sim={similarity:.2f})\"\n",
    "        cv2.putText(frame, arcface_text, (x1, y1 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)\n",
    "\n",
    "    cv2.imshow(window_name, frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
