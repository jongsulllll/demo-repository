{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 사람 + 총 + 칼 인식    \n",
    "2. Arcface를 모든 사람에 대해 적용(Me, NotME)  \n",
    "3. NotMe이면서 사람과 총 혹은 칼의 bounding box가 겹치는 경우에는 위험인으로 분류 -> 한 번 위험인으로 분류된 사람의 ID는 dangerous_ids로 관리됨 -> dangerous_ids 중 하나에 해당하는 사람은 위에 \"Dangerous person\"이라고 뜸\n",
    "4. pose estimation을 dangerous person에 대해 적용(왼팔들기, 오른팔들기, 양팔들기)  \n",
    "5. 출력화면 사이즈를 키움  \n",
    "6. 버그수정: 초기에 무기와 교차하여 dangerous_ids에 들어갔더라도, ArcFace로 내 얼굴임이 확인되면(=나 자신이 무기를 소지한 상황) 그 사람을 위험 인물에서 제외  \n",
    "7. Kobukki robot이 dangerous_ids 내에 있는 사람을 쫓아가도록 하면 될 듯  \n",
    "\n",
    "+ Yolo segmentation까지 추가\n",
    "+ 현재 YOLO를 사용하고 있으므로, YOLOv8-seg 모델을 사용하는 것이 가장 간단함.\n",
    "\n",
    "총이나 칼을 들고 있는 손의 정확한 위치를 파악해서 정밀하게 무기를 감지할 수 있음\n",
    "옷 색깔이나 패턴을 인식해서 사람 구별 정확도 높아짐\n",
    "제스처 구분 정확도 높아짐 \n",
    "YOLOv8-seg 모델 사용\n",
    "\n",
    "\n",
    "v2: seg 모델 학습되어 있는 거 가져와서 테스트 (이전에 학습시켜두었던 yolo 모델은 가져오지 않음). seg + arcface\n",
    "v3: seg + arcface + deepsort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내 얼굴 평균 임베딩 생성 완료.\n"
     ]
    }
   ],
   "source": [
    "#1. 얼굴 학습시키는 부분\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "def initialize_arcface():\n",
    "    app = FaceAnalysis(name=\"buffalo_l\")  # ArcFace 모델 (buffalo_l은 기본 권장)\n",
    "    app.prepare(ctx_id=-1, det_size=(640, 640))  # GPU: ctx_id=0, CPU: -1\n",
    "    return app\n",
    "\n",
    "def get_face_embedding(app, image_bgr):\n",
    "    # ArcFace의 app.get()은 BGR 형식으로 이미지를 받기도 합니다.\n",
    "    # 만약 RGB가 필요하면 cvtColor로 변환하세요.\n",
    "    faces = app.get(image_bgr)\n",
    "    if len(faces) > 0:\n",
    "        return faces[0].embedding  # 첫 번째 얼굴의 임베딩\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_average_embedding(app, folder_path):\n",
    "    embeddings = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                print(f\"이미지 로드 실패: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            embedding = get_face_embedding(app, image)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "            else:\n",
    "                print(f\"얼굴 검출 실패: {img_path}\")\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"임베딩을 하나도 생성하지 못했습니다.\")\n",
    "    \n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    return avg_embedding\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = initialize_arcface()\n",
    "    # 내 얼굴 사진 폴더\n",
    "    my_face_folder = \"C:/myface\"  \n",
    "    my_face_embedding = generate_average_embedding(app, my_face_folder)\n",
    "    np.save(\"my_face_embedding.npy\", my_face_embedding)  # 필요 시 저장\n",
    "    print(\"내 얼굴 평균 임베딩 생성 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "\n",
      "0: 480x640 2 persons, 113.5ms\n",
      "Speed: 10.5ms preprocess, 113.5ms inference, 11.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 142.7ms\n",
      "Speed: 2.4ms preprocess, 142.7ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 115.1ms\n",
      "Speed: 2.0ms preprocess, 115.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 112.4ms\n",
      "Speed: 3.0ms preprocess, 112.4ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 140.0ms\n",
      "Speed: 3.4ms preprocess, 140.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 110.0ms\n",
      "Speed: 3.0ms preprocess, 110.0ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 129.7ms\n",
      "Speed: 3.1ms preprocess, 129.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 160.3ms\n",
      "Speed: 2.4ms preprocess, 160.3ms inference, 7.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 129.5ms\n",
      "Speed: 1.0ms preprocess, 129.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 113.3ms\n",
      "Speed: 3.0ms preprocess, 113.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 144.1ms\n",
      "Speed: 3.5ms preprocess, 144.1ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 143.1ms\n",
      "Speed: 2.4ms preprocess, 143.1ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 111.4ms\n",
      "Speed: 3.1ms preprocess, 111.4ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 persons, 104.9ms\n",
      "Speed: 4.7ms preprocess, 104.9ms inference, 8.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 141.9ms\n",
      "Speed: 3.1ms preprocess, 141.9ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 104.9ms\n",
      "Speed: 2.1ms preprocess, 104.9ms inference, 10.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 138.2ms\n",
      "Speed: 3.1ms preprocess, 138.2ms inference, 7.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 4 persons, 110.1ms\n",
      "Speed: 3.1ms preprocess, 110.1ms inference, 6.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 2 chairs, 108.7ms\n",
      "Speed: 3.1ms preprocess, 108.7ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 122.9ms\n",
      "Speed: 2.1ms preprocess, 122.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 128.6ms\n",
      "Speed: 2.9ms preprocess, 128.6ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 115.6ms\n",
      "Speed: 3.9ms preprocess, 115.6ms inference, 5.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 137.1ms\n",
      "Speed: 2.0ms preprocess, 137.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 116.6ms\n",
      "Speed: 2.2ms preprocess, 116.6ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 285.6ms\n",
      "Speed: 3.3ms preprocess, 285.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 140.0ms\n",
      "Speed: 4.1ms preprocess, 140.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 112.7ms\n",
      "Speed: 3.3ms preprocess, 112.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 110.4ms\n",
      "Speed: 3.1ms preprocess, 110.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 146.4ms\n",
      "Speed: 3.0ms preprocess, 146.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 113.7ms\n",
      "Speed: 3.1ms preprocess, 113.7ms inference, 11.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 1 chair, 114.2ms\n",
      "Speed: 2.6ms preprocess, 114.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "############################\n",
    "# 1) 모델 및 함수 초기화\n",
    "############################\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# YOLO Segmentation 모델 로드\n",
    "segmentation_model = \"yolov8n-seg.pt\"  # 반드시 Segmentation 모델 사용\n",
    "model = YOLO(segmentation_model).to(device)\n",
    "\n",
    "# ArcFace\n",
    "arc_app = FaceAnalysis(name=\"buffalo_l\")\n",
    "arc_app.prepare(ctx_id=-1, det_size=(640, 640))\n",
    "my_face_embedding = np.load(\"my_face_embedding.npy\")\n",
    "\n",
    "# DeepSORT 초기화\n",
    "tracker = DeepSort(max_age=30, n_init=3, embedder='mobilenet', half=True, embedder_gpu=True)\n",
    "\n",
    "# MediaPipe Pose Estimation 모델 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_model = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "############################\n",
    "# 2) 실시간 웹캠 캡처\n",
    "############################\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"웹캠을 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "prev_time = time.time()\n",
    "window_name = \"YOLOv8 Segmentation + DeepSORT (pose) + ArcFace\"\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name, 960, 720)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임 읽기 실패!\")\n",
    "        break\n",
    "\n",
    "    # YOLO Segmentation 추론 실행\n",
    "    results = model(frame)\n",
    "\n",
    "    # 마스크를 적용할 빈 프레임 생성\n",
    "    mask_overlay = np.zeros_like(frame, dtype=np.uint8)\n",
    "\n",
    "    # DeepSORT 입력을 위한 검출된 객체 리스트\n",
    "    detections = []\n",
    "\n",
    "    for result in results:\n",
    "        if result.masks is not None:\n",
    "            for mask, box in zip(result.masks.data, result.boxes):\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # 바운딩 박스 좌표\n",
    "                class_id = int(box.cls)  # 객체 클래스 ID\n",
    "                conf = float(box.conf)  # 신뢰도 점수\n",
    "                class_name = model.names[class_id]  # 객체 이름\n",
    "                mask = mask.cpu().numpy()  # Segmentation 마스크 (0~1 값)\n",
    "\n",
    "                # 바운딩 박스 그리기\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                label = f\"{class_name}: {conf:.2f}\"\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "                # 마스크 크기를 원본 프레임 크기에 맞추기\n",
    "                mask_resized = cv2.resize(mask, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "                # 사람이면 초록색, 무기면 빨간색 마스크 적용\n",
    "                if class_id == 0:  # person\n",
    "                    color = (0, 255, 0)  # 초록색\n",
    "                    detections.append(((x1, y1, x2 - x1, y2 - y1), conf, 0))  # DeepSORT 입력 추가\n",
    "                elif class_id in [1, 2]:  # gun or knife\n",
    "                    color = (0, 0, 255)  # 빨간색\n",
    "                else:\n",
    "                    continue  # 기타 객체는 무시\n",
    "\n",
    "                # 마스크 적용\n",
    "                mask_binary = (mask_resized > 0.5).astype(np.uint8)  # 이진화\n",
    "                color_mask = np.zeros_like(frame, dtype=np.uint8)\n",
    "                color_mask[:, :, 0] = color[0] * mask_binary\n",
    "                color_mask[:, :, 1] = color[1] * mask_binary\n",
    "                color_mask[:, :, 2] = color[2] * mask_binary\n",
    "\n",
    "                # 마스크 오버레이\n",
    "                mask_overlay = cv2.addWeighted(mask_overlay, 1, color_mask, 0.5, 0)\n",
    "\n",
    "    # DeepSORT를 사용하여 객체 추적\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    # 추적된 객체에 대해 ID를 표시 & Pose Estimation 적용\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 1:\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "\n",
    "        # 트래킹된 객체 ID 표시\n",
    "        cv2.putText(frame, f\"ID:{track_id}\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "        # 바운딩 박스 그리기\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 2)\n",
    "\n",
    "        # Pose Estimation 적용 (팔 들기 감지)\n",
    "        person_crop = frame[y1:y2, x1:x2]\n",
    "        if person_crop.size == 0:\n",
    "            continue\n",
    "\n",
    "        crop_rgb = cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB)\n",
    "        pose_result = pose_model.process(crop_rgb)\n",
    "\n",
    "        if pose_result.pose_landmarks:\n",
    "            landmarks = pose_result.pose_landmarks.landmark\n",
    "            left_wrist_y = landmarks[15].y  # 왼손\n",
    "            right_wrist_y = landmarks[16].y  # 오른손\n",
    "            left_shoulder_y = landmarks[11].y  # 왼쪽 어깨\n",
    "            right_shoulder_y = landmarks[12].y  # 오른쪽 어깨\n",
    "\n",
    "            action_text = \"\"\n",
    "            if left_wrist_y < left_shoulder_y - 0.05:\n",
    "                action_text = \"Left arm up\"\n",
    "            if right_wrist_y < right_shoulder_y - 0.05:\n",
    "                action_text = \"Right arm up\"\n",
    "\n",
    "            if action_text:\n",
    "                cv2.putText(frame, action_text, (x1, y1 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # 최종적으로 원본 영상과 마스크를 합성\n",
    "    frame = cv2.addWeighted(frame, 1, mask_overlay, 0.5, 0)\n",
    "\n",
    "    # FPS 계산 및 표시\n",
    "    current_time = time.time()\n",
    "    fps = 1.0 / (current_time - prev_time)\n",
    "    prev_time = current_time\n",
    "    cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # 화면에 결과 표시\n",
    "    cv2.imshow(window_name, frame)\n",
    "\n",
    "    # ESC 또는 'q' 키를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: None\n",
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
