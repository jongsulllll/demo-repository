
import cv2
import numpy as np
import os
import insightface
from insightface.app import FaceAnalysis
from concurrent.futures import ProcessPoolExecutor

def init_model():
    """ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ê°œë³„ì ìœ¼ë¡œ FaceAnalysis ëª¨ë¸ì„ ì´ˆê¸°í™”"""
    global app
    app = FaceAnalysis(name="buffalo_sc")  # MobileFaceNet ê³„ì—´
    app.prepare(ctx_id=0, det_size=(640, 640))  # ğŸ”¹ GPU ëª¨ë“œ ì‚¬ìš©

def get_mobileface_embedding(image_path):
    """ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ì—¬ MobileFaceNet ì„ë² ë”©ì„ ì¶”ì¶œ"""
    img_bgr = cv2.imread(image_path)
    if img_bgr is None:
        return None
    faces = app.get(img_bgr)
    if len(faces) == 0:
        return None
    return faces[0].embedding

def process_image(file, folder):
    """ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë  í•¨ìˆ˜ (my_face_folderë¥¼ ì¸ìë¡œ ë°›ìŒ)"""
    init_model()  # ğŸ”¹ ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ MobileFaceNet ëª¨ë¸ì„ ê°œë³„ì ìœ¼ë¡œ ì´ˆê¸°í™”
    path = os.path.join(folder, file)
    if file.lower().endswith(('.jpg', '.png', '.jpeg')):
        return get_mobileface_embedding(path)
    return None

if __name__ == "__main__":
    my_face_folder = "C:/myface"
    image_files = [file for file in os.listdir(my_face_folder) if file.lower().endswith(('.jpg', '.png', '.jpeg'))]
    
    embeddings = []
    with ProcessPoolExecutor() as executor:
        results = executor.map(process_image, image_files, [my_face_folder] * len(image_files))  # ğŸ”¹ ì¸ìë¡œ ì „ë‹¬

        for emb in results:
            if emb is not None:
                embeddings.append(emb)

    if len(embeddings) == 0:
        raise ValueError("No embeddings generated from MobileFaceNet.")

    avg_embedding = np.mean(embeddings, axis=0)
    print("MobileFaceNet embedding shape:", avg_embedding.shape)
    
    np.save("my_mobileface_embedding.npy", avg_embedding)
    print("Saved my_mobileface_embedding.npy")


import cv2
import numpy as np
import threading
import multiprocessing as mp
from queue import Queue
from sklearn.metrics.pairwise import cosine_similarity
import torch
from ultralytics import YOLO
import pyrealsense2 as rs
import insightface
from insightface.app import FaceAnalysis
import mediapipe as mp_pose
import time

############################
# (A) ìœ„í—˜ì¸ íŒë³„ ê¸°ì¤€
############################
dangerous_ids = set()
face_threshold = 0.5  # ì–¼êµ´ ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ì¤€

############################
# (B) FaceAnalysis ëª¨ë¸ ì´ˆê¸°í™” (ë©€í‹°í”„ë¡œì„¸ì‹± ìš©)
############################
def init_face_model():
    global face_app
    face_app = FaceAnalysis(name="buffalo_sc")
    face_app.prepare(ctx_id=0, det_size=(640, 640))

def get_face_embedding(face_image):
    faces = face_app.get(face_image)
    if faces:
        return faces[0].embedding
    return None

############################
# (C) YOLOë¡œ ì‚¬ëŒ, ì´, ì¹¼ íƒì§€ (ë©€í‹°í”„ë¡œì„¸ì‹±)
############################
def process_detection(input_queue, output_queue, model_path):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = YOLO(model_path).to(device)
    
    while True:
        frame = input_queue.get()
        if frame is None:
            break
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = model(rgb_frame, conf=0.5)
        
        det = results[0]
        boxes = det.boxes
        
        person_detections = []
        weapon_boxes = []
        for i, box in enumerate(boxes):
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            class_id = int(box.cls[0])
            conf = float(box.conf[0])
            if class_id == 0:  # ì‚¬ëŒ
                w, h = x2 - x1, y2 - y1
                person_detections.append(((x1, y1, w, h), conf, i))
            elif class_id in [1, 2]:  # ì´ ë˜ëŠ” ì¹¼
                weapon_boxes.append((x1, y1, x2, y2))
        
        output_queue.put((person_detections, weapon_boxes))

############################
# (D) ìœ„í—˜ì¸ ë¶„ë¥˜ ë¡œì§
############################
def classify_dangerous_person(person_detections, weapon_boxes, face_embeddings, my_embedding, frame):
    global dangerous_ids
    for (bbox, conf, tid), embedding in zip(person_detections, face_embeddings):
        x, y, w, h = bbox
        person_id = hash(tuple(bbox))
        
        # 1. ArcFace ì„ë² ë”© ë¹„êµ (Me vs NotMe)
        similarity = cosine_similarity([embedding], [my_embedding])[0][0]
        is_me = similarity >= face_threshold
        
        # 2. ì´/ì¹¼ê³¼ ê²¹ì¹˜ëŠ”ì§€ ì²´í¬
        overlap = any(x < wx2 and x + w > wx1 and y < wy2 and y + h > wy1 for wx1, wy1, wx2, wy2 in weapon_boxes)
        
        # 3. NotMeì´ë©´ì„œ ë¬´ê¸°ì™€ ê²¹ì¹˜ë©´ ìœ„í—˜ì¸ìœ¼ë¡œ ë“±ë¡
        if not is_me and overlap:
            dangerous_ids.add(person_id)
        
        # 4. Bounding Box ë° Tracking ID í‘œì‹œ
        label = f"ID: {tid}" if person_id not in dangerous_ids else f"Dangerous Person ID: {tid}"
        color = (0, 0, 255) if person_id in dangerous_ids else (0, 255, 0)
        
        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)

############################
# (E) FPS ê³„ì‚°
############################
def draw_fps(frame, start_time):
    fps = 1 / (time.time() - start_time)
    cv2.putText(frame, f"FPS: {fps:.2f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

############################
# (F) ë©”ì¸ ì‹¤í–‰
############################
if __name__ == "__main__":
    # ë‚´ ì–¼êµ´ ì„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸°
    my_embedding = np.load("my_mobileface_embedding.npy")
    
    # í ì„¤ì •
    frame_queue = Queue()
    detection_input_queue = mp.Queue()
    detection_output_queue = mp.Queue()
    face_input_queue = Queue()
    face_output_queue = Queue()
    
    # YOLO í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    detection_process = mp.Process(target=process_detection, args=(detection_input_queue, detection_output_queue, "C:/epoch180.pt"))
    detection_process.start()
    
    # Face ëª¨ë¸ ì´ˆê¸°í™”
    init_face_model()
    
    # RealSense ì„¤ì •
    pipeline = rs.pipeline()
    config = rs.config()
    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
    pipeline.start(config)
    
    while True:
        start_time = time.time()
        frames = pipeline.wait_for_frames()
        color_frame = frames.get_color_frame()
        if not color_frame:
            continue
        frame = np.asanyarray(color_frame.get_data())
        
        # YOLO íƒì§€ ì‹¤í–‰
        detection_input_queue.put(frame)
        person_detections, weapon_boxes = detection_output_queue.get()
        
        # ArcFace ì–¼êµ´ ë¶„ì„
        face_embeddings = [get_face_embedding(frame[y:y+h, x:x+w]) for (x, y, w, h), _, _ in person_detections]
        face_embeddings = [emb for emb in face_embeddings if emb is not None]
        
        # ìœ„í—˜ì¸ ë¶„ë¥˜ ë° í‘œì‹œ
        classify_dangerous_person(person_detections, weapon_boxes, face_embeddings, my_embedding, frame)
        
        # FPS ì¶œë ¥
        draw_fps(frame, start_time)
        
        # í™”ë©´ í‘œì‹œ
        cv2.imshow("RealSense Stream", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    # ì¢…ë£Œ ì²˜ë¦¬
    pipeline.stop()
    detection_process.terminate()
    cv2.destroyAllWindows()
