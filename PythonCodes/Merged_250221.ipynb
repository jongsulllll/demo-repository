{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 합치기  \n",
    "2. depth값을 10*10 box에서 구함(segmentation 적용 방식은 고민해봐야할듯)\n",
    "3. 여러 모델들 실험하다보니 변수명(Facenet, Mobilefacenet 등..)이 이상한 것이 있는데, 모두 Arcface임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import warnings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "import rospy\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from geometry_msgs.msg import Twist\n",
    "\n",
    "# ROS Initialization\n",
    "rospy.init_node('kobuki_follow_person', anonymous=True)\n",
    "cmd_vel_pub = rospy.Publisher('/mobile_base/commands/velocity', Twist, queue_size=10)\n",
    "\n",
    "# PID Parameters\n",
    "TARGET_DISTANCE = 1.0  # Target distance in meters\n",
    "Kp_linear = 0.4   # Proportional gain for linear velocity\n",
    "Ki_linear = 0.01  # Integral gain for linear velocity\n",
    "Kd_linear = 0.1   # Derivative gain for linear velocity\n",
    "\n",
    "Kp_angular = 0.4   # Proportional gain for angular velocity\n",
    "Ki_angular = 0.01  # Integral gain for angular velocity\n",
    "Kd_angular = 0.1   # Derivative gain for angular velocity\n",
    "\n",
    "prev_error_linear = 0\n",
    "integral_linear = 0\n",
    "prev_error_angular = 0\n",
    "integral_angular = 0\n",
    "\n",
    "############################\n",
    "# (A) Arcface (InsightFace) 준비\n",
    "############################\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "app = FaceAnalysis(name=\"buffalo_sc\")  # \"buffalo_sc\"\n",
    "app.prepare(ctx_id=0, det_size=(640,640))  # GPU라면 ctx_id=0, CPU는 -1\n",
    "\n",
    "def get_face_embedding(image_bgr):\n",
    "    if image_bgr is None or image_bgr.size==0:\n",
    "        return None\n",
    "    faces = app.get(image_bgr)\n",
    "    if len(faces)==0:\n",
    "        return None\n",
    "    return faces[0].embedding  # shape 예: (128,)\n",
    "\n",
    "def is_my_face(face_embedding, my_embedding, threshold=0.4):\n",
    "    sim = cosine_similarity([face_embedding], [my_embedding])[0][0]\n",
    "    return (sim > threshold), sim\n",
    "\n",
    "############################\n",
    "# (B) 모델/함수 초기화\n",
    "############################\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)  #경고제거\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = YOLO(\"/home/dev/runs/detect/train55/weights/epoch180.pt\").to(device)\n",
    "model_seg = YOLO(\"yolov8n-seg.pt\").to(device)\n",
    "\n",
    "#임베딩 가져오기\n",
    "my_face_embedding = np.load(\"my_face_embedding.npy\")  # (128,) or (512,) etc.\n",
    "print(\"Camera initializing with\", device, '...')\n",
    "\n",
    "############################\n",
    "# (C) Mediapipe Pose 등 동일\n",
    "############################\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_danger = mp_pose.Pose(\n",
    "    static_image_mode=True,\n",
    "    model_complexity=1,\n",
    "    enable_segmentation=False,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "LEFT_SHOULDER = 11\n",
    "RIGHT_SHOULDER= 12\n",
    "LEFT_WRIST   = 15\n",
    "RIGHT_WRIST  = 16\n",
    "\n",
    "def is_arm_raised(shoulder_y, wrist_y, threshold=0.05):\n",
    "    return wrist_y < (shoulder_y - threshold)\n",
    "\n",
    "def boxes_overlap(boxA, boxB):\n",
    "    (x1A, y1A, x2A, y2A) = boxA\n",
    "    (x1B, y1B, x2B, y2B) = boxB\n",
    "    overlap_x = not (x2A < x1B or x2B < x1A)\n",
    "    overlap_y = not (y2A < y1B or y2B < y1A)\n",
    "    return overlap_x and overlap_y\n",
    "\n",
    "############################\n",
    "# (D) DeepSORT\n",
    "############################\n",
    "tracker = DeepSort(\n",
    "    max_age=30,\n",
    "    n_init=3,\n",
    "    nms_max_overlap=1.0,\n",
    "    embedder='mobilenet',\n",
    "    half=True,\n",
    "    embedder_gpu=True\n",
    ")\n",
    "\n",
    "############################\n",
    "# (E) 기타 변수 설정\n",
    "############################\n",
    "dangerous_ids = set()\n",
    "track_me_status = {}\n",
    "track_arcface_count= {}\n",
    "MAX_ARCFACE_FRAMES= 20\n",
    "sim = 0\n",
    "\n",
    "window_name = \"DeepSORT + YOLO(SEG) + MobileFaceNet + Pose\"\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name, 960,720)\n",
    "\n",
    "# Initialize RealSense pipeline\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "\n",
    "def compute_pid(error, prev_error, integral, Kp, Ki, Kd):\n",
    "    \"\"\" Compute PID velocity command. \"\"\"\n",
    "    integral += error\n",
    "    derivative = error - prev_error\n",
    "    prev_error = error\n",
    "    return (Kp * error) + (Ki * integral) + (Kd * derivative), prev_error, integral\n",
    "\n",
    "def get_center_window_distance(depth_frame, x1, y1, x2, y2, window_size=30):\n",
    "    \"\"\" Get the average distance of a 10x10 window at the center of the bounding box. \"\"\"\n",
    "    # Calculate the center of the bounding box\n",
    "    cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "    \n",
    "    # Define the window around the center\n",
    "    half_window = window_size // 2\n",
    "    window_x1 = max(0, cx - half_window)\n",
    "    window_y1 = max(0, cy - half_window)\n",
    "    window_x2 = min(depth_frame.get_width(), cx + half_window)\n",
    "    window_y2 = min(depth_frame.get_height(), cy + half_window)\n",
    "    \n",
    "    # Get distances for pixels in the 10x10 window\n",
    "    distances = []\n",
    "    for x in range(window_x1, window_x2):\n",
    "        for y in range(window_y1, window_y2):\n",
    "            distance = depth_frame.get_distance(x, y)\n",
    "            if distance > 0:  # Ignore invalid distances\n",
    "                distances.append(distance)\n",
    "\n",
    "    if distances:\n",
    "        return np.mean(distances)\n",
    "    else:\n",
    "        return 0  # Return 0 if no valid distances\n",
    "\n",
    "\n",
    "prev_time = time.time()\n",
    "try:\n",
    "    while not rospy.is_shutdown():\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image2 = color_image  ##출력용\n",
    "\n",
    "        # Run YOLOv8 inference\n",
    "        results = model(color_image, device=device, conf=0.5)\n",
    "        \n",
    "        # YOLO detection\n",
    "        det= results[0]\n",
    "        boxes2 = det.boxes\n",
    "        masks2 = results_seg[0].masks\n",
    "\n",
    "        person_detections= []\n",
    "        weapon_boxes= []\n",
    "        if boxes2 is not None:\n",
    "            for i, box in enumerate(boxes2):\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                class_id = int(box.cls[0])\n",
    "                conf = float(box.conf[0])\n",
    "\n",
    "                cv2.rectangle(color_image2, (x1,y1), (x2,y2), (0,255,0),2)\n",
    "                label = f\"{model.names[class_id]}: {conf:.2f}\"\n",
    "                cv2.putText(color_image2, label,(x1,y1-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,0,0),2)\n",
    "\n",
    "                # 세그 윤곽선(사람만)\n",
    "                if masks2 is not None and class_id==0:\n",
    "                    if i < len(masks2.data):\n",
    "                        single_mask= masks2.data[i].cpu().numpy()\n",
    "                        mask_bin= (single_mask>0.5).astype(np.uint8)\n",
    "                        contours,_= cv2.findContours(mask_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                        cv2.drawContours(color_image2, contours, -1, (0,255,255), 2)\n",
    "\n",
    "                # DeepSORT\n",
    "                if class_id==0:\n",
    "                    w= x2 - x1\n",
    "                    h= y2 - y1\n",
    "                    person_detections.append(((x1,y1,w,h), conf,0))\n",
    "                elif class_id in [1,2]:\n",
    "                    weapon_boxes.append((x1,y1,x2,y2))\n",
    "\n",
    "        # DeepSORT update\n",
    "        tracks= tracker.update_tracks(person_detections, frame=color_image)\n",
    "        tracked_boxes=[]\n",
    "        for t in tracks:\n",
    "            if not t.is_confirmed() or t.time_since_update>1:\n",
    "                continue\n",
    "            tid= t.track_id\n",
    "            l,t_,r,b_ = map(int,t.to_ltrb())\n",
    "            tracked_boxes.append((tid,l,t_,r,b_))\n",
    "            cv2.putText(color_image2,f\"ID:{tid}\", (l-10,t_-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,0.6,(0,255,255),2)\n",
    "\n",
    "        # Arcface 로직\n",
    "        time1 = time.time()\n",
    "        max_arcface_frames=10 # id 별로 초기 10프레임만 얼굴 인식\n",
    "    \n",
    "        for (tid, px1, py1, px2, py2) in tracked_boxes:\n",
    "            if tid not in track_me_status:\n",
    "                track_me_status[tid]= False\n",
    "            if tid not in track_arcface_count:\n",
    "                track_arcface_count[tid]=0\n",
    "\n",
    "            if track_me_status[tid]==False and track_arcface_count[tid]<max_arcface_frames:\n",
    "                print(f\"Running MobileFacenet for ID {tid} (Frame {track_arcface_count[tid]+1}/{MAX_ARCFACE_FRAMES})\")\n",
    "                track_arcface_count[tid]+=1\n",
    "\n",
    "            \n",
    "                # Arcface 임베딩 추출\n",
    "                PAD=10\n",
    "                sub_face= color_image[max(0,py1-PAD): py2+PAD, max(0,px1-PAD): px2+PAD]\n",
    "                if sub_face.size==0:\n",
    "                    continue\n",
    "\n",
    "                emb_image = get_face_embedding(sub_face)\n",
    "                if emb_image is not None:\n",
    "                    same_person, sim = is_my_face(emb_image, my_face_embedding, threshold=0.4)\n",
    "                    if same_person:\n",
    "                        track_me_status[tid]=True\n",
    "\n",
    "\n",
    "            # 시각화\n",
    "            if track_me_status[tid]:\n",
    "                text_arc= f\"          Me(sim={sim:.2f})\"\n",
    "                color=(0,255,0)\n",
    "                if tid in dangerous_ids:\n",
    "                    dangerous_ids.remove(tid)\n",
    "            else:\n",
    "                text_arc= \"           NotMe(sim={sim:.2f})\"\n",
    "                color=(0,0,255)\n",
    "            cv2.putText(color_image2,text_arc,(px1, py1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,0.6,color,2)\n",
    "        time2 = time.time()\n",
    "        print(\"Arcface running time: \", time2-time1)\n",
    "\n",
    "        # 무기 교차 & NotMe => dangerous\n",
    "        for (tid, px1, py1, px2, py2) in tracked_boxes:\n",
    "            if not track_me_status[tid]:\n",
    "                pbox= (px1, py1, px2, py2)\n",
    "                for wb in weapon_boxes:\n",
    "                    if boxes_overlap(pbox, wb):\n",
    "                        dangerous_ids.add(tid)\n",
    "                        break\n",
    "\n",
    "        # Dangerous => Mediapipe pose && PID\n",
    "        for (tid, px1, py1, px2, py2) in tracked_boxes:\n",
    "            if tid in dangerous_ids:    \n",
    "                sub= color_image[py1:py2, px1:px2]\n",
    "                if sub.size==0:\n",
    "                    continue\n",
    "                #Mediapipe\n",
    "                c_rgb= cv2.cvtColor(sub, cv2.COLOR_BGR2RGB)\n",
    "                pose_result= pose_danger.process(c_rgb)\n",
    "                if pose_result.pose_landmarks:\n",
    "                    lms= pose_result.pose_landmarks.landmark\n",
    "                    sub_w= px2 - px1\n",
    "                    sub_h= py2 - py1\n",
    "\n",
    "                    left_shoulder_y= lms[LEFT_SHOULDER].y\n",
    "                    right_shoulder_y= lms[RIGHT_SHOULDER].y\n",
    "                    left_wrist_y= lms[LEFT_WRIST].y\n",
    "                    right_wrist_y= lms[RIGHT_WRIST].y\n",
    "\n",
    "                    la_up= (left_wrist_y< (left_shoulder_y-0.05))\n",
    "                    ra_up= (right_wrist_y<(right_shoulder_y-0.05))\n",
    "                    if la_up and ra_up:\n",
    "                        a_text= \"both arms up\"\n",
    "                    elif la_up:\n",
    "                        a_text= \"left arm up\"\n",
    "                    elif ra_up:\n",
    "                        a_text= \"right arm up\"\n",
    "                    else:\n",
    "                        a_text= \"do nothing\"\n",
    "\n",
    "                    for lm in lms:\n",
    "                        cx= px1+int(lm.x*sub_w)\n",
    "                        cy= py1+int(lm.y*sub_h)\n",
    "                        cv2.circle(color_image2,(cx,cy),3,(0,255,255),-1)\n",
    "                    cv2.putText(color_image2,f\"Dangerous person: {a_text}\",\n",
    "                                (px1,py1+20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,0,255),2)\n",
    "                #PID\n",
    "                box_width = px2 - px1  #add rotate codes to set person bounding box middle match camera middle PID\n",
    "                box_height = py2 - py1\n",
    "                if box_width > 10 and box_height > 10:\n",
    "                    # Calculate the center 10x10 window's distance average\n",
    "                    depth = get_center_window_distance(depth_frame, px1, py1, px2, py2, window_size=30)\n",
    "\n",
    "                    # PID control for linear velocity (distance control)\n",
    "                    error_linear = TARGET_DISTANCE - depth\n",
    "                    velocity_linear, prev_error_linear, integral_linear = compute_pid(\n",
    "                        error_linear, prev_error_linear, integral_linear, Kp_linear, Ki_linear, Kd_linear\n",
    "                    )\n",
    "\n",
    "                    # PID control for angular velocity (angle control)\n",
    "                    image_center_x = color_image.shape[1] // 2\n",
    "                    box_center_x = (px1 + px2) // 2\n",
    "                    error_angular = image_center_x - box_center_x\n",
    "                    angular_velocity, prev_error_angular, integral_angular = compute_pid(\n",
    "                        error_angular, prev_error_angular, integral_angular, Kp_angular, Ki_angular, Kd_angular\n",
    "                    )\n",
    "\n",
    "                    # Limit velocity and angular velocity\n",
    "                    velocity_linear = max(min(velocity_linear, 0.4), -0.4)  # Limit linear speed\n",
    "                    angular_velocity = max(min(angular_velocity, 0.4), -0.4)  # Limit angular speed\n",
    "\n",
    "                    # Publish velocity command\n",
    "                    cmd_vel = Twist()\n",
    "                    cmd_vel.linear.x = velocity_linear\n",
    "                    cmd_vel.angular.z = angular_velocity\n",
    "                    cmd_vel_pub.publish(cmd_vel)\n",
    "\n",
    "                    # Draw bounding box and display distance\n",
    "                    #cv2.rectangle(color_image2, (x1, y1), (x2, y2), (0, 255, 0), 2)   #YOLO단계에서 박스침\n",
    "                    cv2.putText(color_image2, f\"{depth:.2f}m\", (px1, py1 + 20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                else:\n",
    "                    # If the box is too small, stop moving\n",
    "                    cmd_vel_pub.publish(Twist())  # Stop robot\n",
    "                        \n",
    "                break     # Pose detection, PID for 1 person\n",
    "\n",
    "        # FPS 표시\n",
    "        now= time.time()\n",
    "        fps= 1.0/(now - prev_time)\n",
    "        prev_time= now\n",
    "        cv2.putText(color_image2,f\"FPS:{fps:.2f}\",(10,30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)   \n",
    "\n",
    "        # Show detection\n",
    "        cv2.imshow(\"Kobuki Person Following\", color_image2)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    cv2.destroyAllWindows()\n",
    "    cmd_vel_pub.publish(Twist())  # Stop robot before exiting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
