{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. detection : 사람, 총, 칼 (epoch180.pt 사용)\n",
    "2. tracking id 생성\n",
    "3. segmentation : 사람 한정\n",
    "4. Arcface : tracking id 당 한 번씩만 (Me, NotME)\n",
    "5. notme, 사람과 무기의 bounding box가 겹치는 경우 위험인 분류 -> 위험인의 id는 dangerous_ids로 관리됨 (\"dangerous person\" 표시)\n",
    "   (꼬부기는 dangerous person을 추적)\n",
    "7. pose estimation (왼팔들기, 오른팔들기, 양팔들기)  \n",
    "\n",
    "v6: dangerous person 관리하도록 수정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\insightface\\utils\\transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내 얼굴 평균 임베딩 생성 완료.\n"
     ]
    }
   ],
   "source": [
    "#1. 얼굴 학습시키는 부분\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "def initialize_arcface():\n",
    "    app = FaceAnalysis(name=\"buffalo_l\")  # ArcFace 모델 (buffalo_l은 기본 권장)\n",
    "    app.prepare(ctx_id=-1, det_size=(640, 640))  # GPU: ctx_id=0, CPU: -1\n",
    "    return app\n",
    "\n",
    "def get_face_embedding(app, image_bgr):\n",
    "    # ArcFace의 app.get()은 BGR 형식으로 이미지를 받기도 합니다.\n",
    "    # 만약 RGB가 필요하면 cvtColor로 변환하세요.\n",
    "    faces = app.get(image_bgr)\n",
    "    if len(faces) > 0:\n",
    "        return faces[0].embedding  # 첫 번째 얼굴의 임베딩\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_average_embedding(app, folder_path):\n",
    "    embeddings = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                print(f\"이미지 로드 실패: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            embedding = get_face_embedding(app, image)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "            else:\n",
    "                print(f\"얼굴 검출 실패: {img_path}\")\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"임베딩을 하나도 생성하지 못했습니다.\")\n",
    "    \n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    return avg_embedding\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = initialize_arcface()\n",
    "    # 내 얼굴 사진 폴더\n",
    "    my_face_folder = \"C:/myface\"  \n",
    "    my_face_embedding = generate_average_embedding(app, my_face_folder)\n",
    "    np.save(\"my_face_embedding.npy\", my_face_embedding)  # 필요 시 저장\n",
    "    print(\"내 얼굴 평균 임베딩 생성 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:\\Users\\user/.insightface\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "\n",
      "0: 480x640 1 personnnn, 1 knifeeeee, 118.5ms\n",
      "Speed: 9.3ms preprocess, 118.5ms inference, 8.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 139\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# FPS 계산\u001b[39;00m\n\u001b[0;32m    138\u001b[0m current_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 139\u001b[0m fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFPS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfps\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# 화면 표시\u001b[39;00m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import mediapipe as mp\n",
    "\n",
    "############################\n",
    "# 1) 모델 및 함수 초기화\n",
    "############################\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# YOLO Detection 모델 로드\n",
    "detection_model = YOLO(\"C:/epoch180.pt\").to(device)\n",
    "\n",
    "# YOLO Segmentation 모델 로드\n",
    "segmentation_model = YOLO(\"yolov8n-seg.pt\").to(device)\n",
    "\n",
    "# DeepSORT 초기화\n",
    "tracker = DeepSort(max_age=30, n_init=3, embedder='mobilenet', half=True, embedder_gpu=True)\n",
    "\n",
    "# ArcFace 초기화\n",
    "arc_app = FaceAnalysis(name=\"buffalo_l\")\n",
    "arc_app.prepare(ctx_id=-1, det_size=(640, 640))\n",
    "my_face_embedding = np.load(\"my_face_embedding.npy\")\n",
    "\n",
    "# MediaPipe Pose Estimation 초기화\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_model = mp_pose.Pose(static_image_mode=False, model_complexity=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# ArcFace 분석 결과 저장\n",
    "arcface_results = {}  # {track_id: {\"is_me\": bool, \"similarity\": float}}\n",
    "\n",
    "# Dangerous person 관리\n",
    "dangerous_ids = set()\n",
    "\n",
    "def get_face_embedding(face_image, arc_app):\n",
    "    faces = arc_app.get(face_image)\n",
    "    if len(faces) > 0:\n",
    "        return faces[0].embedding\n",
    "    return None\n",
    "\n",
    "def is_my_face(face_embedding, my_face_embedding, threshold=0.4):\n",
    "    if face_embedding is None:\n",
    "        return False, 0.0\n",
    "    similarity = cosine_similarity([face_embedding], [my_face_embedding])[0][0]\n",
    "    return similarity > threshold, similarity\n",
    "\n",
    "def boxes_overlap(boxA, boxB):\n",
    "    x1A, y1A, x2A, y2A = boxA\n",
    "    x1B, y1B, x2B, y2B = boxB\n",
    "    return not (x2A < x1B or x2B < x1A or y2A < y1B or y2B < y1A)\n",
    "\n",
    "############################\n",
    "# 2) 실시간 웹캡 캡처\n",
    "############################\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"웹캠을 열 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "window_name = \"YOLOv8 + DeepSORT + Dangerous Person\"\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name, 960, 720)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"프레임 읽기 실패!\")\n",
    "        break\n",
    "\n",
    "    # YOLOv8 Detection 실행\n",
    "    detection_results = detection_model(frame)\n",
    "    detections = []\n",
    "    weapon_boxes = []  # 무기 위치 저장\n",
    "\n",
    "    for box in detection_results[0].boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        class_id = int(box.cls)\n",
    "        conf = float(box.conf)\n",
    "        label = f\"{detection_model.names[class_id]}: {conf:.2f}\"\n",
    "\n",
    "        if class_id == 0:  # 사람\n",
    "            color = (0, 255, 0)  # 초록색\n",
    "            detections.append(((x1, y1, x2 - x1, y2 - y1), conf, 0))\n",
    "        elif class_id in [1, 2]:  # 무기 (총, 칼)\n",
    "            color = (0, 0, 255)  # 빨간색\n",
    "            weapon_boxes.append((x1, y1, x2, y2))  # 무기 위치 저장\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # 바운딩 박스 그리기\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "\n",
    "    # DeepSORT 추적\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed() or track.time_since_update > 1:\n",
    "            continue\n",
    "\n",
    "        track_id = track.track_id\n",
    "        x1, y1, x2, y2 = map(int, track.to_ltrb())\n",
    "\n",
    "        # Tracking ID 표시\n",
    "        cv2.putText(frame, f\"ID:{track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "        # ArcFace (한 번만 실행)\n",
    "        if track_id not in arcface_results:\n",
    "            person_crop = frame[y1:y2, x1:x2]\n",
    "            face_embedding = get_face_embedding(person_crop, arc_app)\n",
    "            is_me, similarity = is_my_face(face_embedding, my_face_embedding)\n",
    "            arcface_results[track_id] = {\"is_me\": is_me, \"similarity\": similarity}\n",
    "\n",
    "        arcface_data = arcface_results[track_id]\n",
    "        if arcface_data[\"is_me\"]:\n",
    "            arc_text = f\"Me (sim={arcface_data['similarity']:.2f})\"\n",
    "            cv2.putText(frame, arc_text, (x1, y1 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        else:\n",
    "            arc_text = f\"Not Me (sim={arcface_data['similarity']:.2f})\"\n",
    "            cv2.putText(frame, arc_text, (x1, y1 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Dangerous person 체크\n",
    "        person_box = (x1, y1, x2, y2)\n",
    "        for weapon_box in weapon_boxes:\n",
    "            if boxes_overlap(person_box, weapon_box):\n",
    "                dangerous_ids.add(track_id)\n",
    "                break\n",
    "\n",
    "        if track_id in dangerous_ids:\n",
    "            cv2.putText(frame, \"Dangerous Person\", (x1, y1 + 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Pose Estimation (팔 들기 감지)\n",
    "        person_crop_rgb = cv2.cvtColor(frame[y1:y2, x1:x2], cv2.COLOR_BGR2RGB)\n",
    "        pose_result = pose_model.process(person_crop_rgb)\n",
    "\n",
    "        if pose_result.pose_landmarks:\n",
    "            landmarks = pose_result.pose_landmarks.landmark\n",
    "            left_wrist_y = landmarks[15].y\n",
    "            right_wrist_y = landmarks[16].y\n",
    "            left_shoulder_y = landmarks[11].y\n",
    "            right_shoulder_y = landmarks[12].y\n",
    "\n",
    "            if left_wrist_y < left_shoulder_y - 0.05:\n",
    "                cv2.putText(frame, \"Left arm up\", (x1, y1 + 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            if right_wrist_y < right_shoulder_y - 0.05:\n",
    "                cv2.putText(frame, \"Right arm up\", (x1, y1 + 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "\n",
    "    \n",
    "    # FPS 계산\n",
    "    current_time = time.time()\n",
    "    fps = 1.0 / (current_time - time.time())\n",
    "    cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # 화면 표시\n",
    "    cv2.imshow(window_name, frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
