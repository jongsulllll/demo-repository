{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사람 + 총 + 칼 인식\n",
    "pose estimation은 모든 사람에 대해 인식(수정해서 아래 경우에만 적용할 수도 있음)\n",
    "사람과 총 혹은 칼의 bounding box가 겹치는 경우에만 arcface 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#1. 얼굴 학습시키는 부분\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "#1. 얼굴 학습시키는 부분\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "def initialize_arcface():\n",
    "    app = FaceAnalysis(name=\"buffalo_l\")  # ArcFace 모델 (buffalo_l은 기본 권장)\n",
    "    app.prepare(ctx_id=-1, det_size=(640, 640))  # GPU: ctx_id=0, CPU: -1\n",
    "    return app\n",
    "\n",
    "def get_face_embedding(app, image_bgr):\n",
    "    # ArcFace의 app.get()은 BGR 형식으로 이미지를 받기도 합니다.\n",
    "    # 만약 RGB가 필요하면 cvtColor로 변환하세요.\n",
    "    faces = app.get(image_bgr)\n",
    "    if len(faces) > 0:\n",
    "        return faces[0].embedding  # 첫 번째 얼굴의 임베딩\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_average_embedding(app, folder_path):\n",
    "    embeddings = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                print(f\"이미지 로드 실패: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            embedding = get_face_embedding(app, image)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "            else:\n",
    "                print(f\"얼굴 검출 실패: {img_path}\")\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"임베딩을 하나도 생성하지 못했습니다.\")\n",
    "    \n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "    return avg_embedding\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = initialize_arcface()\n",
    "    # 내 얼굴 사진 폴더\n",
    "    my_face_folder = \"C:/myface\"  \n",
    "    my_face_embedding = generate_average_embedding(app, my_face_folder)\n",
    "    np.save(\"my_face_embedding.npy\", my_face_embedding)  # 필요 시 저장\n",
    "    print(\"내 얼굴 평균 임베딩 생성 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import rospy\n",
    "from std_msgs.msg import Bool\n",
    "from ultralytics import YOLO\n",
    "from insightface.app import FaceAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "\n",
    "# ================================\n",
    "# ROS 초기화\n",
    "# ================================\n",
    "rospy.init_node('pose_emergency_detector', anonymous=True)\n",
    "emergency_pub = rospy.Publisher('/is_emergency', Bool, queue_size=1)\n",
    "\n",
    "# ================================\n",
    "# 모델 초기화\n",
    "# ================================\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# YOLOv8 불러오기\n",
    "model = YOLO(\"C:/epoch180.pt\").to(device)\n",
    "\n",
    "# ArcFace 설정\n",
    "arc_app = FaceAnalysis(name='buffalo_l')\n",
    "arc_app.prepare(ctx_id=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_model = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# ================================\n",
    "# 카메라 열기\n",
    "# ================================\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def is_arm_raised(landmarks):\n",
    "    \"\"\"팔이 어깨보다 위로 올라가면 True\"\"\"\n",
    "    left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "    right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "    left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "    right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "\n",
    "    left_up = left_wrist.y < left_shoulder.y\n",
    "    right_up = right_wrist.y < right_shoulder.y\n",
    "    return left_up or right_up\n",
    "\n",
    "while not rospy.is_shutdown():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # YOLO 감지\n",
    "    results = model(frame)[0]\n",
    "\n",
    "    # 위험 판별 변수 초기화\n",
    "    is_emergency = False\n",
    "\n",
    "    # 얼굴 추출\n",
    "    faces = arc_app.get(rgb_frame)\n",
    "    embeddings = [f['embedding'] for f in faces]\n",
    "\n",
    "    # 포즈 추정\n",
    "    pose_results = pose_model.process(rgb_frame)\n",
    "    if pose_results.pose_landmarks:\n",
    "        mp_draw.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        landmarks = pose_results.pose_landmarks.landmark\n",
    "        if is_arm_raised(landmarks):\n",
    "            is_emergency = True\n",
    "\n",
    "    # ROS publish\n",
    "    emergency_pub.publish(Bool(data=is_emergency))\n",
    "\n",
    "    # 디버깅용 표시\n",
    "    if is_emergency:\n",
    "        cv2.putText(frame, \"EMERGENCY\", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,0,255), 3)\n",
    "\n",
    "    cv2.imshow(\"Pose + Face\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
